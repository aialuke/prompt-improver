#!/usr/bin/env python3
"""
Comprehensive validation tests for security hardening changes.
Tests input validation, memory guards, cache management, and performance monitoring.
"""

import sys
import numpy as np
import asyncio
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from prompt_improver.security import InputValidator, MemoryGuard, ValidationError
    from prompt_improver.ml.learning.algorithms.context_cache_manager import ContextCacheManager
    from prompt_improver.ml.learning.algorithms.context_performance_monitor import ContextPerformanceMonitor
    from prompt_improver.ml.orchestration.integration.component_invoker import ComponentInvoker, InvocationResult
    from prompt_improver.ml.learning.algorithms.context_learner import ContextSpecificLearner, ContextConfig
except ImportError as e:
    print(f"Import error: {e}")
    print("Make sure you're running from project root")
    sys.exit(1)

import logging
logging.basicConfig(level=logging.INFO)


class SecurityValidationTests:
    """Comprehensive security validation test suite."""

    def __init__(self):
        self.test_results = []
        self.logger = logging.getLogger(__name__)

    def log_test(self, test_name: str, passed: bool, message: str = ""):
        """Log test result."""
        status = "✅ PASS" if passed else "❌ FAIL"
        full_message = f"{status} {test_name}"
        if message:
            full_message += f" - {message}"

        print(full_message)
        self.test_results.append((test_name, passed, message))

    def test_input_validator(self):
        """Test input validation components."""
        print("\n=== Testing Input Validator ===")

        try:
            validator = InputValidator()

            # Test valid session ID
            try:
                result = validator.validate_input("session_id", "valid_session_123")
                self.log_test("Valid session ID validation", True, f"Accepted: {result}")
            except ValidationError:
                self.log_test("Valid session ID validation", False, "Incorrectly rejected valid session")

            # Test invalid session IDs
            invalid_sessions = [
                "",  # Empty
                "ab",  # Too short
                "x" * 200,  # Too long
                "<script>alert('xss')</script>",  # XSS
                None,  # None value
                123,  # Wrong type
            ]

            for i, invalid_session in enumerate(invalid_sessions):
                try:
                    validator.validate_input("session_id", invalid_session)
                    self.log_test(f"Invalid session rejection {i+1}", False, f"Incorrectly accepted: {invalid_session}")
                except ValidationError:
                    self.log_test(f"Invalid session rejection {i+1}", True, f"Correctly rejected: {type(invalid_session).__name__}")

            # Test valid context data
            valid_context = {
                "domain": "web_development",
                "projectType": "web",
                "complexity": "medium",
                "teamSize": 5
            }
            try:
                result = validator.validate_input("context_data", valid_context)
                self.log_test("Valid context data validation", True)
            except ValidationError as e:
                self.log_test("Valid context data validation", False, str(e))

            # Test numpy array validation
            valid_array = np.array([1.0, 2.0, 3.0], dtype=np.float64)
            try:
                result = validator.validate_input("numpy_array", valid_array)
                self.log_test("Valid numpy array validation", True, f"Shape: {result.shape}")
            except ValidationError as e:
                self.log_test("Valid numpy array validation", False, str(e))

            # Test invalid numpy array (NaN values)
            invalid_array = np.array([1.0, np.nan, 3.0])
            try:
                validator.validate_input("numpy_array", invalid_array)
                self.log_test("Invalid numpy array rejection", False, "Incorrectly accepted NaN array")
            except ValidationError:
                self.log_test("Invalid numpy array rejection", True, "Correctly rejected NaN array")

        except Exception as e:
            self.log_test("Input Validator initialization", False, str(e))

    def test_memory_guard(self):
        """Test memory guard components."""
        print("\n=== Testing Memory Guard ===")

        try:
            # Test with reasonable memory limits
            memory_guard = MemoryGuard(max_memory_mb=100, max_buffer_size=10*1024*1024)

            # Test buffer size validation
            small_buffer = b"test data"
            try:
                result = memory_guard.validate_buffer_size(small_buffer, "test")
                self.log_test("Small buffer validation", True, f"Size: {len(small_buffer)} bytes")
            except MemoryError:
                self.log_test("Small buffer validation", False, "Incorrectly rejected small buffer")

            # Test oversized buffer
            large_buffer = b"x" * (20 * 1024 * 1024)  # 20MB
            try:
                memory_guard.validate_buffer_size(large_buffer, "test")
                self.log_test("Large buffer rejection", False, "Incorrectly accepted oversized buffer")
            except MemoryError:
                self.log_test("Large buffer rejection", True, f"Correctly rejected {len(large_buffer)} byte buffer")

            # Test safe numpy operations
            test_array = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float64)
            buffer = test_array.tobytes()

            try:
                result = memory_guard.safe_frombuffer(buffer, np.float64)
                np.testing.assert_array_equal(result, test_array)
                self.log_test("Safe frombuffer operation", True, f"Restored array shape: {result.shape}")
            except (ValueError, MemoryError) as e:
                self.log_test("Safe frombuffer operation", False, str(e))

            try:
                result_bytes = memory_guard.safe_tobytes(test_array)
                self.log_test("Safe tobytes operation", True, f"Generated {len(result_bytes)} bytes")
            except MemoryError as e:
                self.log_test("Safe tobytes operation", False, str(e))

            # Test memory monitoring
            stats = memory_guard.check_memory_usage()
            expected_keys = ["current_mb", "peak_mb", "usage_percent"]
            has_all_keys = all(key in stats for key in expected_keys)
            self.log_test("Memory monitoring", has_all_keys, f"Stats keys: {list(stats.keys())}")

        except Exception as e:
            self.log_test("Memory Guard initialization", False, str(e))

    def test_cache_manager(self):
        """Test cache manager LRU functionality."""
        print("\n=== Testing Cache Manager ===")

        try:
            # Test with small cache sizes for easy validation
            cache_manager = ContextCacheManager(linguistic_cache_size=3, domain_cache_size=2)

            # Test linguistic cache LRU
            cache_manager.update_linguistic_cache("key1", "value1")
            cache_manager.update_linguistic_cache("key2", "value2")
            cache_manager.update_linguistic_cache("key3", "value3")

            # Cache should be at capacity
            stats = cache_manager.get_cache_stats()
            self.log_test("Linguistic cache at capacity", stats["linguistic_cache_size"] == 3)

            # Adding one more should evict the oldest (key1)
            cache_manager.update_linguistic_cache("key4", "value4")

            # key1 should be evicted, key4 should be present
            key1_result = cache_manager.get_linguistic_cache("key1")
            key4_result = cache_manager.get_linguistic_cache("key4")

            self.log_test("LRU eviction works", key1_result is None and key4_result == "value4")

            # Test domain cache LRU
            cache_manager.update_domain_cache("domain1", "dvalue1")
            cache_manager.update_domain_cache("domain2", "dvalue2")
            cache_manager.update_domain_cache("domain3", "dvalue3")  # Should evict domain1

            domain1_result = cache_manager.get_domain_cache("domain1")
            domain3_result = cache_manager.get_domain_cache("domain3")

            self.log_test("Domain cache LRU eviction", domain1_result is None and domain3_result == "dvalue3")

            # Test cache clearing
            cleared_stats = cache_manager.clear_all_caches()
            final_stats = cache_manager.get_cache_stats()

            caches_cleared = (
                final_stats["linguistic_cache_size"] == 0 and
                final_stats["domain_cache_size"] == 0 and
                cleared_stats["linguistic_cleared"] > 0
            )
            self.log_test("Cache clearing", caches_cleared, f"Cleared: {cleared_stats}")

        except Exception as e:
            self.log_test("Cache Manager testing", False, str(e))

    def test_performance_monitor(self):
        """Test performance monitoring functionality."""
        print("\n=== Testing Performance Monitor ===")

        try:
            memory_guard = MemoryGuard()
            monitor = ContextPerformanceMonitor(memory_guard)

            # Test operation tracking
            monitor.track_operation("test_operation", 0.05)  # 50ms
            monitor.track_operation("test_operation", 0.03)  # 30ms
            monitor.track_operation("another_operation", 0.10)  # 100ms

            # Test privacy budget tracking
            monitor.update_privacy_budget(1.5)
            monitor.update_privacy_budget(0.8)

            # Get comprehensive metrics
            cache_stats = {"linguistic_cache_size": 5, "domain_cache_size": 3}
            metrics = monitor.get_performance_metrics(cache_stats, context_clusters_count=2)

            expected_metrics = [
                "memory_usage_mb", "memory_peak_mb", "memory_usage_percent",
                "context_clusters", "privacy_budget_used", "operation_counts",
                "linguistic_cache_size", "domain_cache_size"
            ]

            has_all_metrics = all(key in metrics for key in expected_metrics)
            self.log_test("Performance metrics completeness", has_all_metrics, f"Keys: {list(metrics.keys())}")

            # Verify operation counts
            operation_counts_correct = (
                metrics["operation_counts"]["test_operation"] == 2 and
                metrics["operation_counts"]["another_operation"] == 1
            )
            self.log_test("Operation tracking", operation_counts_correct, f"Counts: {metrics['operation_counts']}")

            # Verify privacy budget
            budget_correct = abs(metrics["privacy_budget_used"] - 2.3) < 0.001
            self.log_test("Privacy budget tracking", budget_correct, f"Budget used: {metrics['privacy_budget_used']}")

            # Test privacy budget status
            budget_status = monitor.get_privacy_budget_status()
            budget_status_complete = all(key in budget_status for key in ["budget_used", "budget_remaining", "budget_utilization"])
            self.log_test("Privacy budget status", budget_status_complete, f"Status: {budget_status}")

        except Exception as e:
            self.log_test("Performance Monitor testing", False, str(e))

    async def test_component_invoker(self):
        """Test simplified ComponentInvoker."""
        print("\n=== Testing Component Invoker ===")

        try:
            # Create a mock component loader
            class MockComponentLoader:
                def get_loaded_component(self, name):
                    if name == "test_component":
                        class MockComponent:
                            def __init__(self):
                                self.instance = self
                                self.is_initialized = True

                            def test_method(self):
                                return "test_result"

                            async def async_test_method(self):
                                return "async_test_result"

                        return MockComponent()
                    return None

            invoker = ComponentInvoker(MockComponentLoader())

            # Test successful method invocation
            result = await invoker.invoke_component_method("test_component", "test_method")
            success_test = result.success and result.result == "test_result"
            self.log_test("Component method invocation", success_test, f"Result: {result.result}")

            # Test async method invocation
            async_result = await invoker.invoke_component_method("test_component", "async_test_method")
            async_success = async_result.success and async_result.result == "async_test_result"
            self.log_test("Async component method invocation", async_success, f"Result: {async_result.result}")

            # Test non-existent component
            missing_result = await invoker.invoke_component_method("missing_component", "test_method")
            missing_test = not missing_result.success and "not loaded" in missing_result.error
            self.log_test("Missing component handling", missing_test, f"Error: {missing_result.error}")

            # Test non-existent method
            method_result = await invoker.invoke_component_method("test_component", "missing_method")
            method_test = not method_result.success and "not found" in method_result.error
            self.log_test("Missing method handling", method_test, f"Error: {method_result.error}")

            # Test performance metrics
            history = invoker.get_invocation_history(limit=5)
            success_rate = invoker.get_success_rate()

            history_test = len(history) > 0
            self.log_test("Invocation history tracking", history_test, f"History entries: {len(history)}")

            rate_test = 0.0 <= success_rate <= 1.0
            self.log_test("Success rate calculation", rate_test, f"Success rate: {success_rate}")

        except Exception as e:
            self.log_test("Component Invoker testing", False, str(e))

    async def run_all_tests(self):
        """Run all validation tests."""
        print("🧪 Starting Security Validation Tests")
        print("=" * 50)

        self.test_input_validator()
        self.test_memory_guard()
        self.test_cache_manager()
        self.test_performance_monitor()
        await self.test_component_invoker()

        # Summary
        print("\n" + "=" * 50)
        print("📊 Test Summary")
        print("=" * 50)

        passed_tests = sum(1 for _, passed, _ in self.test_results if passed)
        total_tests = len(self.test_results)
        pass_rate = passed_tests / total_tests if total_tests > 0 else 0

        print(f"Passed: {passed_tests}/{total_tests} ({pass_rate:.1%})")

        if pass_rate < 1.0:
            print("\n❌ Failed Tests:")
            for test_name, passed, message in self.test_results:
                if not passed:
                    print(f"   • {test_name}: {message}")
        else:
            print("\n✅ All tests passed!")

        return pass_rate == 1.0


if __name__ == "__main__":
    tester = SecurityValidationTests()
    all_passed = asyncio.run(tester.run_all_tests())
    sys.exit(0 if all_passed else 1)
