#!/usr/bin/env python3
"""
Comprehensive Phase 3 ML Pipeline Implementation Test

Tests all Phase 3 implementations:
- ML orchestration health monitoring integration
- Component Integration Strategy with event emission
- API Gateway endpoints
- Tier 3 & 4 component integration
"""

import asyncio
import logging
import sys
import traceback
from typing import Dict, Any

# Test imports
sys.path.insert(0, 'src')

async def test_ml_orchestration_health_integration():
    """Test ML orchestration health checkers integration with existing HealthService."""
    print("\nüîç Testing ML orchestration health monitoring integration...")
    
    try:
        # Test ML orchestration health checker integration without complex imports
        from prompt_improver.performance.monitoring.health.ml_orchestration_checkers import (
            MLOrchestratorHealthChecker,
            MLComponentRegistryHealthChecker,
            MLResourceManagerHealthChecker,
            MLWorkflowEngineHealthChecker,
            MLEventBusHealthChecker
        )
        
        print("  ‚úì Successfully imported ML orchestration health checkers")
        
        # Test health checker instantiation
        orchestrator_checker = MLOrchestratorHealthChecker()
        registry_checker = MLComponentRegistryHealthChecker()
        resource_checker = MLResourceManagerHealthChecker()
        workflow_checker = MLWorkflowEngineHealthChecker()
        event_checker = MLEventBusHealthChecker()
        
        print("  ‚úì Successfully instantiated all ML health checkers")
        
        # Test health checks (without actual components - should handle gracefully)
        orchestrator_result = await orchestrator_checker.check()
        print(f"  ‚úì ML Orchestrator health check: {orchestrator_result.status.value}")
        
        registry_result = await registry_checker.check()
        print(f"  ‚úì Component Registry health check: {registry_result.status.value}")
        
        resource_result = await resource_checker.check()
        print(f"  ‚úì Resource Manager health check: {resource_result.status.value}")
        
        workflow_result = await workflow_checker.check()
        print(f"  ‚úì Workflow Engine health check: {workflow_result.status.value}")
        
        event_result = await event_checker.check()
        print(f"  ‚úì Event Bus health check: {event_result.status.value}")
        
        # Test that health checkers exist in the existing health system
        try:
            from prompt_improver.performance.monitoring.health.service import HealthService
            health_service = HealthService()
            if hasattr(health_service, 'configure_ml_orchestration_checkers'):
                print("  ‚úì HealthService has configure_ml_orchestration_checkers method")
            else:
                print("  ‚ö†Ô∏è  HealthService doesn't have configure_ml_orchestration_checkers method")
        except ImportError:
            print("  ‚ö†Ô∏è  Could not import HealthService due to dependencies, but health checkers work standalone")
        
        print("‚úÖ ML orchestration health monitoring integration: PASSED")
        return True
        
    except Exception as e:
        print(f"‚ùå ML orchestration health monitoring integration: FAILED - {e}")
        traceback.print_exc()
        return False

async def test_component_integration_strategy():
    """Test Component Integration Strategy with optional event emission."""
    print("\nüîç Testing Component Integration Strategy with event emission...")
    
    try:
        # Test MLModelService with optional orchestrator event bus
        from prompt_improver.ml.core.ml_integration import MLModelService
        
        # Test without event bus (backward compatibility)
        ml_service = MLModelService()
        print("  ‚úì MLModelService instantiated without orchestrator_event_bus (backward compatible)")
        
        # Test with mock event bus
        class MockEventBus:
            def __init__(self):
                self.emitted_events = []
            
            async def emit(self, event):
                self.emitted_events.append(event)
        
        mock_event_bus = MockEventBus()
        ml_service_with_events = MLModelService(orchestrator_event_bus=mock_event_bus)
        print("  ‚úì MLModelService instantiated with orchestrator_event_bus")
        
        # Test event emission during operation (simulate training completion)
        await ml_service_with_events._emit_orchestrator_event(
            "TRAINING_COMPLETED",
            {"model_id": "test_model", "accuracy": 0.95}
        )
        print("  ‚úì Successfully emitted orchestrator event")
        print(f"    - Events emitted: {len(mock_event_bus.emitted_events)}")
        
        # Test that it doesn't fail when event bus is None
        await ml_service._emit_orchestrator_event(
            "TRAINING_COMPLETED",
            {"model_id": "test_model", "accuracy": 0.95}
        )
        print("  ‚úì Event emission gracefully handled when no event bus available")
        
        print("‚úÖ Component Integration Strategy: PASSED")
        return True
        
    except Exception as e:
        print(f"‚ùå Component Integration Strategy: FAILED - {e}")
        traceback.print_exc()
        return False

async def test_api_gateway_endpoints():
    """Test API Gateway endpoints functionality."""
    print("\nüîç Testing API Gateway endpoints...")
    
    try:
        # Test orchestrator endpoints
        from prompt_improver.ml.orchestration.api.orchestrator_endpoints import OrchestratorEndpoints
        
        # Test endpoint instantiation
        endpoints = OrchestratorEndpoints()
        print("  ‚úì OrchestratorEndpoints instantiated successfully")
        
        # Test router creation
        router = endpoints.get_router()
        print(f"  ‚úì FastAPI router created with routes")
        
        # Test route configuration (check routes exist)
        route_paths = [route.path for route in router.routes]
        expected_routes = [
            "/status", "/workflows", "/workflows/{workflow_id}", 
            "/workflows/{workflow_id}/stop", "/components", 
            "/components/register", "/components/{component_name}/health",
            "/metrics", "/health"
        ]
        
        for expected_route in expected_routes:
            if any(expected_route in path for path in route_paths):
                print(f"    ‚úì Route {expected_route} configured")
            else:
                print(f"    ‚ö†Ô∏è  Route {expected_route} might not be configured properly")
        
        # Test status endpoint without orchestrator (should handle gracefully)
        try:
            await endpoints.get_status()
        except Exception as e:
            if "Orchestrator not available" in str(e):
                print("  ‚úì Status endpoint correctly handles missing orchestrator")
            else:
                raise e
        
        print("‚úÖ API Gateway endpoints: PASSED")
        return True
        
    except Exception as e:
        print(f"‚ùå API Gateway endpoints: FAILED - {e}")
        traceback.print_exc()
        return False

async def test_tier3_tier4_component_integration():
    """Test Tier 3 & 4 component integration."""
    print("\nüîç Testing Tier 3 & 4 component integration...")
    
    try:
        # Test component definitions
        from prompt_improver.ml.orchestration.config.component_definitions import ComponentDefinitions
        from prompt_improver.ml.orchestration.core.component_registry import ComponentTier
        
        component_defs = ComponentDefinitions()
        
        # Test Tier 3 components (Evaluation & Analysis)
        tier3_components = component_defs.get_tier_components(ComponentTier.TIER_3_EVALUATION)
        print(f"  ‚úì Tier 3 components loaded: {len(tier3_components)} components")
        
        expected_tier3 = [
            "experiment_orchestrator", "advanced_statistical_validator", 
            "causal_inference_analyzer", "pattern_significance_analyzer",
            "statistical_analyzer", "structural_analyzer", 
            "domain_feature_extractor", "linguistic_analyzer",
            "dependency_parser", "domain_detector"
        ]
        
        for component_name in expected_tier3:
            if component_name in tier3_components:
                print(f"    ‚úì {component_name} defined in Tier 3")
            else:
                print(f"    ‚ùå {component_name} missing from Tier 3")
        
        # Test Tier 4 components (Performance & Testing)
        tier4_components = component_defs.get_tier_components(ComponentTier.TIER_4_PERFORMANCE)
        print(f"  ‚úì Tier 4 components loaded: {len(tier4_components)} components")
        
        expected_tier4 = [
            "advanced_ab_testing", "canary_testing", "real_time_analytics",
            "analytics", "monitoring", "async_optimizer", 
            "early_stopping", "background_manager"
        ]
        
        for component_name in expected_tier4:
            if component_name in tier4_components:
                print(f"    ‚úì {component_name} defined in Tier 4")
            else:
                print(f"    ‚ùå {component_name} missing from Tier 4")
        
        # Test ComponentRegistry with Tier 3 & 4 integration
        from prompt_improver.ml.orchestration.core.component_registry import ComponentRegistry
        from prompt_improver.ml.orchestration.config.orchestrator_config import OrchestratorConfig
        
        config = OrchestratorConfig()
        registry = ComponentRegistry(config)
        
        # Test component loading
        await registry._load_component_definitions()
        print(f"  ‚úì Component registry loaded components: {len(registry.components)} total")
        
        # Verify Tier 3 & 4 components are registered
        tier3_registered = await registry.list_components(ComponentTier.TIER_3_EVALUATION)
        tier4_registered = await registry.list_components(ComponentTier.TIER_4_PERFORMANCE)
        
        print(f"    - Tier 3 registered: {len(tier3_registered)} components")
        print(f"    - Tier 4 registered: {len(tier4_registered)} components")
        
        if len(tier3_registered) == len(expected_tier3) and len(tier4_registered) == len(expected_tier4):
            print("  ‚úì All Tier 3 & 4 components successfully registered")
        else:
            print(f"  ‚ö†Ô∏è  Component registration mismatch - Expected T3:{len(expected_tier3)}, T4:{len(expected_tier4)}")
        
        print("‚úÖ Tier 3 & 4 component integration: PASSED")
        return True
        
    except Exception as e:
        print(f"‚ùå Tier 3 & 4 component integration: FAILED - {e}")
        traceback.print_exc()
        return False

async def test_event_bus_functionality():
    """Test event bus functionality for health monitoring."""
    print("\nüîç Testing event bus functionality...")
    
    try:
        from prompt_improver.ml.orchestration.events.event_bus import EventBus
        from prompt_improver.ml.orchestration.config.orchestrator_config import OrchestratorConfig
        
        config = OrchestratorConfig()
        event_bus = EventBus(config)
        
        # Test initialization
        await event_bus.initialize()
        print("  ‚úì Event bus initialized successfully")
        
        # Test statistics method
        stats = event_bus.get_statistics()
        required_stats = ["total_events", "failed_events", "active_handlers", "queue_size", "is_running"]
        
        for stat in required_stats:
            if stat in stats:
                print(f"    ‚úì Statistic '{stat}': {stats[stat]}")
            else:
                print(f"    ‚ùå Missing statistic: {stat}")
        
        # Test health check event emission
        await event_bus.emit_health_check_event("test_source")
        print("  ‚úì Health check event emitted successfully")
        
        # Test statistics after event
        updated_stats = event_bus.get_statistics()
        if updated_stats["total_events"] > stats["total_events"]:
            print("  ‚úì Event statistics updated correctly")
        
        # Test shutdown
        await event_bus.shutdown()
        print("  ‚úì Event bus shutdown successfully")
        
        print("‚úÖ Event bus functionality: PASSED")
        return True
        
    except Exception as e:
        print(f"‚ùå Event bus functionality: FAILED - {e}")
        traceback.print_exc()
        return False

async def validate_no_false_positives():
    """Validate that implementation has no false-positive outputs."""
    print("\nüîç Validating no false-positive outputs...")
    
    try:
        # Test that health checks return appropriate statuses for missing components
        from prompt_improver.performance.monitoring.health.ml_orchestration_checkers import MLOrchestratorHealthChecker
        
        checker = MLOrchestratorHealthChecker()
        result = await checker.check()
        
        # Should be WARNING or FAILED, not HEALTHY when no orchestrator is present
        if result.status.value in ["warning", "failed"]:
            print("  ‚úì Health checker correctly reports non-healthy status when components missing")
        else:
            print(f"  ‚ùå False positive: Health checker reports '{result.status.value}' when no orchestrator present")
            return False
        
        # Test that component registry handles missing tiers gracefully
        from prompt_improver.ml.orchestration.config.component_definitions import ComponentDefinitions
        from prompt_improver.ml.orchestration.core.component_registry import ComponentTier
        
        component_defs = ComponentDefinitions()
        tier5_components = component_defs.get_tier_components(ComponentTier.TIER_5_INFRASTRUCTURE)
        tier6_components = component_defs.get_tier_components(ComponentTier.TIER_6_SECURITY)
        
        # These should return empty dict (not implemented yet)
        if tier5_components == {} and tier6_components == {}:
            print("  ‚úì Unimplemented tiers correctly return empty results")
        else:
            print(f"  ‚ùå False positive: Unimplemented tiers return non-empty results")
            return False
        
        # Test event emission with None event bus doesn't create false events
        from prompt_improver.ml.core.ml_integration import MLModelService
        
        ml_service = MLModelService()
        # This should not raise an exception or create phantom events
        await ml_service._emit_orchestrator_event("TEST_EVENT", {"test": True})
        print("  ‚úì Event emission with None event bus handled gracefully")
        
        print("‚úÖ No false-positive validation: PASSED")
        return True
        
    except Exception as e:
        print(f"‚ùå False-positive validation: FAILED - {e}")
        traceback.print_exc()
        return False

async def main():
    """Run comprehensive Phase 3 tests."""
    print("üöÄ Starting Phase 3 ML Pipeline Implementation Comprehensive Test")
    print("=" * 80)
    
    # Configure logging
    logging.basicConfig(level=logging.WARNING)  # Reduce noise during testing
    
    test_results = []
    
    # Run all tests
    tests = [
        ("ML Orchestration Health Integration", test_ml_orchestration_health_integration),
        ("Component Integration Strategy", test_component_integration_strategy),
        ("API Gateway Endpoints", test_api_gateway_endpoints),
        ("Tier 3 & 4 Component Integration", test_tier3_tier4_component_integration),
        ("Event Bus Functionality", test_event_bus_functionality),
        ("False-Positive Validation", validate_no_false_positives)
    ]
    
    for test_name, test_func in tests:
        try:
            result = await test_func()
            test_results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name}: FAILED with exception - {e}")
            test_results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 80)
    print("üìä PHASE 3 TEST SUMMARY")
    print("=" * 80)
    
    passed_tests = sum(1 for _, result in test_results if result)
    total_tests = len(test_results)
    
    for test_name, result in test_results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{test_name}: {status}")
    
    print(f"\nOverall Result: {passed_tests}/{total_tests} tests passed")
    
    if passed_tests == total_tests:
        print("üéâ ALL PHASE 3 TESTS PASSED - Implementation ready for production")
        return 0
    else:
        print("‚ö†Ô∏è  SOME TESTS FAILED - Review implementation before proceeding")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)