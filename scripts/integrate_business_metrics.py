"""Integration Script for Business Metrics in Prompt Improver Application.

This script demonstrates how to integrate the comprehensive business metrics system
into the existing FastAPI application to collect real business insights from
actual operations.
"""

import asyncio
import logging
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
logger = logging.getLogger(__name__)


async def integrate_business_metrics():
    """Integrate business metrics into the existing application.
    This shows how to add metrics collection to a running FastAPI app.
    """
    logger.info("Integrating business metrics into Prompt Improver application...")
    try:
        from prompt_improver.metrics.application_instrumentation import (
            instrument_application_startup,
        )
        from prompt_improver.monitoring.http.unified_http_middleware import (
            UnifiedHTTPMetricsMiddleware,
        )

        logger.info("Step 1: Initializing metrics collection system...")
        await initialize_metrics_collection()
        logger.info("Step 2: Instrumenting existing application components...")
        instrument_application_startup()
        logger.info("Step 3: Creating FastAPI middleware integration...")
        create_fastapi_integration_example()
        logger.info("Step 4: Creating database integration example...")
        create_database_integration_example()
        logger.info("Step 5: Creating ML service integration example...")
        create_ml_service_integration_example()
        logger.info("Step 6: Creating cost tracking integration...")
        create_cost_tracking_integration()
        logger.info("Step 7: Creating dashboard setup...")
        create_dashboard_setup()
        logger.info("Business metrics integration completed successfully!")
        display_integration_summary()
    except Exception as e:
        logger.error("Failed to integrate business metrics: %s", e)
        raise


def create_fastapi_integration_example():
    """Create example showing how to integrate with FastAPI application."""
    fastapi_integration_code = '\n"""\nFastAPI Application with Business Metrics Integration.\nAdd this to your main FastAPI application file.\n"""\n\nfrom fastapi import FastAPI, Request, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nimport asyncio\n\n# Import business metrics components\nfrom prompt_improver.metrics.integration_middleware import (\n    BusinessMetricsMiddleware,\n    initialize_metrics_collection,\n    shutdown_metrics_collection\n)\nfrom prompt_improver.metrics.application_instrumentation import (\n    track_ml_operation,\n    track_feature_usage,\n    track_cost_operation\n)\nfrom prompt_improver.metrics import (\n    PromptCategory,\n    FeatureCategory,\n    UserTier,\n    CostType\n)\n\n# Create FastAPI app\napp = FastAPI(title="Prompt Improver with Business Metrics")\n\n# Add business metrics middleware\napp.add_middleware(BusinessMetricsMiddleware)\n\n# Add CORS middleware (if needed)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=["*"],\n    allow_credentials=True,\n    allow_methods=["*"],\n    allow_headers=["*"],\n)\n\n@app.on_event("startup")\nasync def startup_event():\n    """Initialize business metrics on application startup."""\n    await initialize_metrics_collection()\n    logger.info("Business metrics collection started")\n\n@app.on_event("shutdown")\nasync def shutdown_event():\n    """Shutdown business metrics on application shutdown."""\n    await shutdown_metrics_collection()\n    logger.info("Business metrics collection stopped")\n\n# Example API endpoints with automatic metrics collection\n@app.post("/api/v1/prompt/improve")\n@track_feature_usage(\n    feature_name="prompt_improvement_api",\n    feature_category=FeatureCategory.PROMPT_ENHANCEMENT\n)\n@track_cost_operation(\n    operation_type="prompt_improvement",\n    cost_type=CostType.ML_INFERENCE,\n    estimated_cost_per_unit=0.01\n)\nasync def improve_prompt(request: dict, user_id: str = None):\n    """Improve prompt with automatic metrics collection."""\n    # Your existing prompt improvement logic here\n    # Metrics are automatically collected by decorators and middleware\n    \n    result = {\n        "improved_prompt": f"Improved: {request.get(\'prompt\', \'\')}",\n        "confidence": 0.85,\n        "success": True\n    }\n    \n    return result\n\n@app.get("/api/v1/dashboard/metrics")\nasync def get_metrics_dashboard():\n    """Get real-time metrics dashboard data."""\n    from prompt_improver.metrics import get_dashboard_exporter, TimeRange, ExportFormat\n    \n    dashboard_exporter = get_dashboard_exporter()\n    \n    # Get executive summary\n    executive_summary = await dashboard_exporter.export_executive_summary(\n        time_range=TimeRange.LAST_HOUR,\n        export_format=ExportFormat.JSON\n    )\n    \n    return executive_summary\n\nif __name__ == "__main__":\n    import uvicorn\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n'
    with open(project_root / "examples" / "fastapi_metrics_integration.py", "w") as f:
        f.write(fastapi_integration_code)
    logger.info(
        "Created FastAPI integration example: examples/fastapi_metrics_integration.py"
    )


def create_database_integration_example():
    """Create example showing how to integrate with database operations."""
    database_integration_code = '\n"""\nDatabase Integration with Business Metrics.\nShows how to automatically track database performance metrics.\n"""\n\nimport asyncio\nimport time\nfrom typing import Any, List, Dict\nfrom datetime import datetime, timezone\n\nfrom prompt_improver.metrics.integration_middleware import db_metrics\nfrom prompt_improver.metrics.performance_metrics import DatabaseOperation\n\nclass MetricsEnabledDatabase:\n    """\n    Database class with automatic metrics collection.\n    Wrap your existing database operations with this pattern.\n    """\n    \n    def __init__(self, connection_pool):\n        self.pool = connection_pool\n    \n    async def execute_query(self, query: str, params: tuple = None) -> Any:\n        """Execute query with automatic metrics tracking."""\n        start_time = time.time()\n        \n        # Determine operation type\n        query_lower = query.lower().strip()\n        if query_lower.startswith(\'select\'):\n            operation_type = DatabaseOperation.SELECT\n        elif query_lower.startswith(\'insert\'):\n            operation_type = DatabaseOperation.INSERT\n        elif query_lower.startswith(\'update\'):\n            operation_type = DatabaseOperation.UPDATE\n        elif query_lower.startswith(\'delete\'):\n            operation_type = DatabaseOperation.DELETE\n        else:\n            operation_type = DatabaseOperation.SELECT  # Default\n        \n        # Extract table name\n        table_name = self._extract_table_name(query)\n        \n        try:\n            # Execute actual query\n            async with self.pool.acquire() as connection:\n                result = await connection.fetch(query, *params if params else ())\n            \n            success = True\n            error_type = None\n            rows_affected = len(result) if isinstance(result, list) else 1\n            \n        except Exception as e:\n            result = None\n            success = False\n            error_type = type(e).__name__\n            rows_affected = 0\n            raise\n        finally:\n            # Track database metrics\n            end_time = time.time()\n            execution_time_ms = (end_time - start_time) * 1000\n            \n            await db_metrics.track_query(\n                query=query,\n                operation_type=operation_type,\n                table_name=table_name,\n                execution_time_ms=execution_time_ms,\n                rows_affected=rows_affected,\n                success=success,\n                error_type=error_type\n            )\n        \n        return result\n    \n    def _extract_table_name(self, query: str) -> str:\n        """Extract table name from SQL query."""\n        query_lower = query.lower()\n        \n        # Simple table extraction logic\n        if \'from \' in query_lower:\n            parts = query_lower.split(\'from \')\n            if len(parts) > 1:\n                table_part = parts[1].split()[0]\n                return table_part.strip(\'`"[]\')\n        \n        for keyword in [\'insert into \', \'update \', \'delete from \']:\n            if keyword in query_lower:\n                parts = query_lower.split(keyword)\n                if len(parts) > 1:\n                    return parts[1].split()[0].strip(\'`"[]\')\n        \n        return "unknown"\n\n# Example usage\nasync def example_database_usage():\n    """Example of using database with metrics."""\n    # Assuming you have a connection pool\n    # db = MetricsEnabledDatabase(your_connection_pool)\n    \n    # All these operations will automatically generate metrics\n    # users = await db.execute_query("SELECT * FROM users WHERE active = $1", (True,))\n    # await db.execute_query("INSERT INTO sessions (user_id, created_at) VALUES ($1, $2)", (user_id, datetime.now()))\n    # await db.execute_query("UPDATE users SET last_login = $1 WHERE id = $2", (datetime.now(), user_id))\n    \n    pass\n'
    examples_dir = project_root / "examples"
    examples_dir.mkdir(exist_ok=True)
    with open(examples_dir / "database_metrics_integration.py", "w") as f:
        f.write(database_integration_code)
    logger.info(
        "Created database integration example: examples/database_metrics_integration.py"
    )


def create_ml_service_integration_example():
    """Create example showing how to integrate with ML services."""
    ml_integration_code = '\n"""\nML Service Integration with Business Metrics.\nShows how to automatically track ML operations and business intelligence.\n"""\n\nimport asyncio\nimport time\nfrom typing import Dict, Any, List\nfrom datetime import datetime, timezone\n\nfrom prompt_improver.metrics.integration_middleware import (\n    track_ml_operation,\n    track_feature_usage,\n    track_cost_operation\n)\nfrom prompt_improver.metrics import (\n    PromptCategory,\n    ModelInferenceStage,\n    FeatureCategory,\n    UserTier,\n    CostType,\n    record_model_inference\n)\n\nclass PromptImprovementService:\n    """\n    Enhanced prompt improvement service with comprehensive metrics.\n    """\n    \n    @track_ml_operation(\n        category=PromptCategory.CLARITY,\n        stage=ModelInferenceStage.MODEL_FORWARD,\n        model_name="gpt4_prompt_optimizer"\n    )\n    @track_feature_usage(\n        feature_name="advanced_prompt_optimization",\n        feature_category=FeatureCategory.PROMPT_ENHANCEMENT\n    )\n    @track_cost_operation(\n        operation_type="ml_prompt_optimization",\n        cost_type=CostType.ML_INFERENCE,\n        estimated_cost_per_unit=0.05  # $0.05 per optimization\n    )\n    async def improve_prompt_with_ai(\n        self, \n        prompt: str, \n        user_id: str = None, \n        session_id: str = None,\n        user_tier: UserTier = UserTier.FREE\n    ) -> Dict[str, Any]:\n        """\n        Improve prompt using AI with comprehensive metrics tracking.\n        All metrics are automatically collected by decorators.\n        """\n        start_time = time.time()\n        \n        try:\n            # Simulate AI model inference\n            # In real implementation, this would call your actual ML model\n            improved_prompt = f"Enhanced prompt: {prompt}"\n            confidence = 0.87\n            \n            # Simulate processing time based on prompt complexity\n            processing_time = len(prompt) * 0.01 + 0.5  # Base time + complexity\n            await asyncio.sleep(processing_time)\n            \n            # Record detailed model inference metrics\n            await record_model_inference(\n                model_name="gpt4_prompt_optimizer",\n                inference_stage=ModelInferenceStage.MODEL_FORWARD,\n                input_tokens=len(prompt.split()),\n                output_tokens=len(improved_prompt.split()),\n                latency_ms=(time.time() - start_time) * 1000,\n                memory_usage_mb=150,  # Estimated memory usage\n                success=True,\n                confidence_distribution=[confidence]\n            )\n            \n            return {\n                "original_prompt": prompt,\n                "improved_prompt": improved_prompt,\n                "confidence": confidence,\n                "improvements": [\n                    "Enhanced clarity",\n                    "Added context",\n                    "Optimized structure"\n                ],\n                "model_used": "gpt4_prompt_optimizer",\n                "processing_time_ms": (time.time() - start_time) * 1000\n            }\n            \n        except Exception as e:\n            # Record failed inference\n            await record_model_inference(\n                model_name="gpt4_prompt_optimizer",\n                inference_stage=ModelInferenceStage.MODEL_FORWARD,\n                input_tokens=len(prompt.split()),\n                output_tokens=0,\n                latency_ms=(time.time() - start_time) * 1000,\n                memory_usage_mb=0,\n                success=False,\n                error_type=type(e).__name__\n            )\n            raise\n    \n    @track_feature_usage(\n        feature_name="batch_prompt_processing",\n        feature_category=FeatureCategory.BATCH_PROCESSING\n    )\n    async def process_batch_prompts(\n        self, \n        prompts: List[str], \n        user_id: str = None\n    ) -> Dict[str, Any]:\n        """Process multiple prompts in batch with metrics."""\n        results = []\n        \n        for prompt in prompts:\n            try:\n                result = await self.improve_prompt_with_ai(\n                    prompt=prompt,\n                    user_id=user_id\n                )\n                results.append(result)\n            except Exception as e:\n                results.append({\n                    "original_prompt": prompt,\n                    "error": str(e),\n                    "success": False\n                })\n        \n        return {\n            "total_prompts": len(prompts),\n            "successful_improvements": len([r for r in results if r.get("success", True)]),\n            "results": results\n        }\n\nclass MLAnalyticsService:\n    """ML analytics service with business intelligence metrics."""\n    \n    @track_feature_usage(\n        feature_name="ml_performance_analysis",\n        feature_category=FeatureCategory.ML_ANALYTICS,\n        user_tier=UserTier.PROFESSIONAL\n    )\n    async def analyze_model_performance(\n        self, \n        model_name: str, \n        time_period_hours: int = 24\n    ) -> Dict[str, Any]:\n        """Analyze ML model performance with BI metrics."""\n        # Get ML metrics collector\n        from prompt_improver.metrics import get_ml_metrics_collector\n        \n        ml_collector = get_ml_metrics_collector()\n        \n        # Get model performance summary\n        performance_summary = await ml_collector.get_model_performance_summary(\n            hours=time_period_hours\n        )\n        \n        return {\n            "model_name": model_name,\n            "analysis_period_hours": time_period_hours,\n            "performance_data": performance_summary,\n            "recommendations": [\n                "Optimize input preprocessing",\n                "Consider model quantization",\n                "Implement response caching"\n            ]\n        }\n\n# Example usage\nasync def example_ml_service_usage():\n    """Example of using ML services with comprehensive metrics."""\n    prompt_service = PromptImprovementService()\n    analytics_service = MLAnalyticsService()\n    \n    # Single prompt improvement (automatically tracked)\n    result = await prompt_service.improve_prompt_with_ai(\n        prompt="Help me write better code",\n        user_id="user123",\n        session_id="session456",\n        user_tier=UserTier.PROFESSIONAL\n    )\n    \n    # Batch processing (automatically tracked)\n    batch_result = await prompt_service.process_batch_prompts(\n        prompts=[\n            "Explain machine learning",\n            "Write a business plan",\n            "Create a marketing strategy"\n        ],\n        user_id="user123"\n    )\n    \n    # Performance analysis (automatically tracked)\n    analysis = await analytics_service.analyze_model_performance(\n        model_name="gpt4_prompt_optimizer",\n        time_period_hours=24\n    )\n    \n    return {\n        "single_improvement": result,\n        "batch_processing": batch_result,\n        "performance_analysis": analysis\n    }\n'
    with open(
        project_root / "examples" / "ml_service_metrics_integration.py", "w"
    ) as f:
        f.write(ml_integration_code)
    logger.info(
        "Created ML service integration example: examples/ml_service_metrics_integration.py"
    )


def create_cost_tracking_integration():
    """Create example showing how to implement cost tracking."""
    cost_tracking_code = '\n"""\nCost Tracking Integration for Business Intelligence.\nShows how to track operational costs across different services.\n"""\n\nimport asyncio\nimport time\nfrom typing import Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom enum import Enum\n\nfrom prompt_improver.metrics import (\n    record_operational_cost,\n    CostType,\n    UserTier\n)\n\nclass ResourceUsageTracker:\n    """\n    Track resource usage and convert to costs for business intelligence.\n    """\n    \n    # Cost rates (in USD)\n    COST_RATES = {\n        CostType.COMPUTE: 0.001,      # $0.001 per CPU second\n        CostType.STORAGE: 0.00001,    # $0.00001 per MB-hour\n        CostType.NETWORK: 0.0001,     # $0.0001 per MB transferred\n        CostType.ML_INFERENCE: 0.01,  # $0.01 per inference call\n        CostType.EXTERNAL_API: 0.005, # $0.005 per API call\n        CostType.DATABASE: 0.002,     # $0.002 per query\n    }\n    \n    async def track_compute_cost(\n        self, \n        operation_type: str,\n        cpu_seconds: float,\n        user_id: Optional[str] = None,\n        user_tier: Optional[UserTier] = None\n    ):\n        """Track compute resource costs."""\n        cost_amount = cpu_seconds * self.COST_RATES[CostType.COMPUTE]\n        \n        await record_operational_cost(\n            operation_type=operation_type,\n            cost_type=CostType.COMPUTE,\n            cost_amount=cost_amount,\n            resource_units_consumed=cpu_seconds,\n            resource_unit_cost=self.COST_RATES[CostType.COMPUTE],\n            user_id=user_id,\n            user_tier=user_tier,\n            service_name="compute_service",\n            allocation_tags={\n                "resource_type": "cpu",\n                "operation": operation_type\n            }\n        )\n    \n    async def track_ml_inference_cost(\n        self,\n        model_name: str,\n        inference_count: int,\n        user_id: Optional[str] = None,\n        user_tier: Optional[UserTier] = None\n    ):\n        """Track ML inference costs."""\n        cost_amount = inference_count * self.COST_RATES[CostType.ML_INFERENCE]\n        \n        await record_operational_cost(\n            operation_type=f"ml_inference_{model_name}",\n            cost_type=CostType.ML_INFERENCE,\n            cost_amount=cost_amount,\n            resource_units_consumed=inference_count,\n            resource_unit_cost=self.COST_RATES[CostType.ML_INFERENCE],\n            user_id=user_id,\n            user_tier=user_tier,\n            service_name="ml_service",\n            allocation_tags={\n                "model": model_name,\n                "inference_type": "prompt_improvement"\n            }\n        )\n    \n    async def track_storage_cost(\n        self,\n        storage_mb_hours: float,\n        storage_type: str = "general",\n        user_id: Optional[str] = None\n    ):\n        """Track storage costs."""\n        cost_amount = storage_mb_hours * self.COST_RATES[CostType.STORAGE]\n        \n        await record_operational_cost(\n            operation_type=f"storage_{storage_type}",\n            cost_type=CostType.STORAGE,\n            cost_amount=cost_amount,\n            resource_units_consumed=storage_mb_hours,\n            resource_unit_cost=self.COST_RATES[CostType.STORAGE],\n            user_id=user_id,\n            service_name="storage_service",\n            allocation_tags={\n                "storage_type": storage_type\n            }\n        )\n    \n    async def track_network_cost(\n        self,\n        data_transfer_mb: float,\n        transfer_type: str = "outbound",\n        user_id: Optional[str] = None\n    ):\n        """Track network transfer costs."""\n        cost_amount = data_transfer_mb * self.COST_RATES[CostType.NETWORK]\n        \n        await record_operational_cost(\n            operation_type=f"network_{transfer_type}",\n            cost_type=CostType.NETWORK,\n            cost_amount=cost_amount,\n            resource_units_consumed=data_transfer_mb,\n            resource_unit_cost=self.COST_RATES[CostType.NETWORK],\n            user_id=user_id,\n            service_name="network_service",\n            allocation_tags={\n                "transfer_type": transfer_type\n            }\n        )\n\nclass CostAwareService:\n    """\n    Example service that automatically tracks its operational costs.\n    """\n    \n    def __init__(self):\n        self.cost_tracker = ResourceUsageTracker()\n    \n    async def expensive_ml_operation(\n        self, \n        data: Dict[str, Any],\n        user_id: str = None,\n        user_tier: UserTier = UserTier.FREE\n    ) -> Dict[str, Any]:\n        """Perform expensive ML operation with cost tracking."""\n        start_time = time.time()\n        \n        try:\n            # Simulate ML processing\n            await asyncio.sleep(2.0)  # 2 seconds of processing\n            \n            # Track compute cost\n            cpu_seconds = 2.0\n            await self.cost_tracker.track_compute_cost(\n                operation_type="expensive_ml_operation",\n                cpu_seconds=cpu_seconds,\n                user_id=user_id,\n                user_tier=user_tier\n            )\n            \n            # Track ML inference cost\n            await self.cost_tracker.track_ml_inference_cost(\n                model_name="expensive_model_v1",\n                inference_count=1,\n                user_id=user_id,\n                user_tier=user_tier\n            )\n            \n            # Track network cost (response size)\n            response_size_mb = 5.0  # 5MB response\n            await self.cost_tracker.track_network_cost(\n                data_transfer_mb=response_size_mb,\n                transfer_type="api_response",\n                user_id=user_id\n            )\n            \n            return {\n                "result": "Operation completed successfully",\n                "processing_time": time.time() - start_time,\n                "cost_breakdown": {\n                    "compute": cpu_seconds * ResourceUsageTracker.COST_RATES[CostType.COMPUTE],\n                    "ml_inference": ResourceUsageTracker.COST_RATES[CostType.ML_INFERENCE],\n                    "network": response_size_mb * ResourceUsageTracker.COST_RATES[CostType.NETWORK]\n                }\n            }\n            \n        except Exception as e:\n            # Still track costs for failed operations\n            cpu_seconds = time.time() - start_time\n            await self.cost_tracker.track_compute_cost(\n                operation_type="expensive_ml_operation_failed",\n                cpu_seconds=cpu_seconds,\n                user_id=user_id,\n                user_tier=user_tier\n            )\n            raise\n\n# Cost monitoring and alerting\nclass CostMonitor:\n    """Monitor costs and provide alerts."""\n    \n    async def get_user_cost_summary(self, user_id: str, hours: int = 24) -> Dict[str, Any]:\n        """Get cost summary for a specific user."""\n        from prompt_improver.metrics import get_bi_metrics_collector\n        \n        bi_collector = get_bi_metrics_collector()\n        cost_report = await bi_collector.get_cost_efficiency_report(days=hours//24 or 1)\n        \n        return {\n            "user_id": user_id,\n            "time_period_hours": hours,\n            "cost_summary": cost_report,\n            "recommendations": [\n                "Consider upgrading to Professional tier for better rates",\n                "Optimize batch operations to reduce per-request costs",\n                "Use caching to reduce redundant ML inferences"\n            ]\n        }\n    \n    async def check_cost_alerts(self, threshold_usd: float = 100.0) -> List[Dict[str, Any]]:\n        """Check for cost alerts based on thresholds."""\n        # In a real implementation, this would check against cost thresholds\n        # and send alerts when exceeded\n        \n        alerts = []\n        \n        # Example alert\n        alerts.append({\n            "type": "cost_threshold_exceeded",\n            "message": f"Daily costs exceeded ${threshold_usd}",\n            "current_cost": 125.50,\n            "threshold": threshold_usd,\n            "recommended_action": "Review usage patterns and consider optimization"\n        })\n        \n        return alerts\n\n# Example usage\nasync def example_cost_tracking():\n    """Example of comprehensive cost tracking."""\n    cost_service = CostAwareService()\n    cost_monitor = CostMonitor()\n    \n    # Perform operation with automatic cost tracking\n    result = await cost_service.expensive_ml_operation(\n        data={"operation": "complex_analysis"},\n        user_id="user123",\n        user_tier=UserTier.PROFESSIONAL\n    )\n    \n    # Get cost summary\n    cost_summary = await cost_monitor.get_user_cost_summary(\n        user_id="user123",\n        hours=24\n    )\n    \n    # Check for cost alerts\n    alerts = await cost_monitor.check_cost_alerts(threshold_usd=50.0)\n    \n    return {\n        "operation_result": result,\n        "cost_summary": cost_summary,\n        "cost_alerts": alerts\n    }\n'
    with open(project_root / "examples" / "cost_tracking_integration.py", "w") as f:
        f.write(cost_tracking_code)
    logger.info(
        "Created cost tracking integration example: examples/cost_tracking_integration.py"
    )


def create_dashboard_setup():
    """Create dashboard setup and configuration."""
    dashboard_setup_code = '\n"""\nDashboard Setup for Business Metrics Visualization.\nProvides real-time dashboard endpoints and data export functionality.\n"""\n\nimport asyncio\nfrom typing import Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom fastapi import APIRouter, Query, HTTPException\nfrom fastapi.responses import JSONResponse, StreamingResponse\nimport json\nimport io\n\nfrom prompt_improver.metrics import (\n    get_dashboard_exporter,\n    get_aggregation_engine,\n    get_ml_metrics_collector,\n    get_api_metrics_collector,\n    get_performance_metrics_collector,\n    get_bi_metrics_collector,\n    ExportFormat,\n    DashboardType,\n    TimeRange\n)\n\n# Create dashboard router\ndashboard_router = APIRouter(prefix="/dashboard", tags=["dashboard"])\n\n@dashboard_router.get("/executive-summary")\nasync def get_executive_summary(\n    time_range: str = Query("last_hour", description="Time range: last_hour, last_day, last_week"),\n    format: str = Query("json", description="Export format: json, csv, excel")\n):\n    """Get executive summary dashboard."""\n    try:\n        dashboard_exporter = get_dashboard_exporter()\n        \n        # Map string to enum\n        time_range_enum = {\n            "last_hour": TimeRange.LAST_HOUR,\n            "last_day": TimeRange.LAST_DAY,\n            "last_week": TimeRange.LAST_WEEK\n        }.get(time_range, TimeRange.LAST_HOUR)\n        \n        format_enum = {\n            "json": ExportFormat.JSON,\n            "csv": ExportFormat.CSV,\n            "excel": ExportFormat.EXCEL\n        }.get(format, ExportFormat.JSON)\n        \n        data = await dashboard_exporter.export_executive_summary(\n            time_range=time_range_enum,\n            export_format=format_enum\n        )\n        \n        if format_enum == ExportFormat.JSON:\n            return JSONResponse(content=data)\n        elif format_enum == ExportFormat.CSV:\n            return StreamingResponse(\n                io.StringIO(data),\n                media_type="text/csv",\n                headers={"Content-Disposition": "attachment; filename=executive_summary.csv"}\n            )\n        else:\n            return StreamingResponse(\n                io.BytesIO(data),\n                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",\n                headers={"Content-Disposition": "attachment; filename=executive_summary.xlsx"}\n            )\n            \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f"Failed to generate executive summary: {e}")\n\n@dashboard_router.get("/ml-performance")\nasync def get_ml_performance(\n    time_range: str = Query("last_hour"),\n    format: str = Query("json")\n):\n    """Get ML performance dashboard."""\n    try:\n        dashboard_exporter = get_dashboard_exporter()\n        \n        time_range_enum = {\n            "last_hour": TimeRange.LAST_HOUR,\n            "last_day": TimeRange.LAST_DAY,\n            "last_week": TimeRange.LAST_WEEK\n        }.get(time_range, TimeRange.LAST_HOUR)\n        \n        format_enum = {\n            "json": ExportFormat.JSON,\n            "csv": ExportFormat.CSV,\n            "excel": ExportFormat.EXCEL\n        }.get(format, ExportFormat.JSON)\n        \n        data = await dashboard_exporter.export_ml_performance(\n            time_range=time_range_enum,\n            export_format=format_enum\n        )\n        \n        return JSONResponse(content=data)\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f"Failed to generate ML performance dashboard: {e}")\n\n@dashboard_router.get("/real-time-monitoring")\nasync def get_real_time_monitoring():\n    """Get real-time monitoring dashboard."""\n    try:\n        dashboard_exporter = get_dashboard_exporter()\n        \n        data = await dashboard_exporter.export_real_time_monitoring(\n            export_format=ExportFormat.JSON\n        )\n        \n        return JSONResponse(content=data)\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f"Failed to generate real-time dashboard: {e}")\n\n@dashboard_router.get("/business-insights")\nasync def get_business_insights():\n    """Get AI-powered business insights."""\n    try:\n        aggregation_engine = get_aggregation_engine()\n        insights = await aggregation_engine.get_business_insights()\n        \n        return JSONResponse(content=insights)\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f"Failed to generate business insights: {e}")\n\n@dashboard_router.get("/metrics-summary")\nasync def get_metrics_summary():\n    """Get summary of all metrics collection statistics."""\n    try:\n        # Get all collectors\n        ml_collector = get_ml_metrics_collector()\n        api_collector = get_api_metrics_collector()\n        performance_collector = get_performance_metrics_collector()\n        bi_collector = get_bi_metrics_collector()\n        \n        summary = {\n            "collection_timestamp": datetime.now(timezone.utc).isoformat(),\n            "ml_metrics": ml_collector.get_collection_stats(),\n            "api_metrics": api_collector.get_collection_stats(),\n            "performance_metrics": performance_collector.get_collection_stats(),\n            "business_intelligence": bi_collector.get_collection_stats()\n        }\n        \n        return JSONResponse(content=summary)\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f"Failed to get metrics summary: {e}")\n\n@dashboard_router.get("/feature-adoption")\nasync def get_feature_adoption(days: int = Query(7, description="Number of days to analyze")):\n    """Get feature adoption analysis."""\n    try:\n        bi_collector = get_bi_metrics_collector()\n        adoption_report = await bi_collector.get_feature_adoption_report(days=days)\n        \n        return JSONResponse(content=adoption_report)\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f"Failed to generate feature adoption report: {e}")\n\n@dashboard_router.get("/cost-efficiency")\nasync def get_cost_efficiency(days: int = Query(7, description="Number of days to analyze")):\n    """Get cost efficiency analysis."""\n    try:\n        bi_collector = get_bi_metrics_collector()\n        cost_report = await bi_collector.get_cost_efficiency_report(days=days)\n        \n        return JSONResponse(content=cost_report)\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f"Failed to generate cost efficiency report: {e}")\n\n# WebSocket endpoint for real-time updates\n@dashboard_router.websocket("/real-time-updates")\nasync def real_time_updates_websocket(websocket):\n    """WebSocket endpoint for real-time dashboard updates."""\n    await websocket.accept()\n    \n    try:\n        dashboard_exporter = get_dashboard_exporter()\n        \n        while True:\n            # Get latest real-time data\n            data = await dashboard_exporter.export_real_time_monitoring(\n                export_format=ExportFormat.JSON\n            )\n            \n            # Send data to client\n            await websocket.send_json({\n                "timestamp": datetime.now(timezone.utc).isoformat(),\n                "data": data\n            })\n            \n            # Wait before next update\n            await asyncio.sleep(5)  # Update every 5 seconds\n            \n    except Exception as e:\n        await websocket.close(code=1000, reason=f"Error: {e}")\n\n# Health check for dashboard services\n@dashboard_router.get("/health")\nasync def dashboard_health():\n    """Health check for dashboard services."""\n    try:\n        # Check if all collectors are working\n        ml_collector = get_ml_metrics_collector()\n        api_collector = get_api_metrics_collector()\n        performance_collector = get_performance_metrics_collector()\n        bi_collector = get_bi_metrics_collector()\n        \n        health_status = {\n            "status": "healthy",\n            "timestamp": datetime.now(timezone.utc).isoformat(),\n            "collectors": {\n                "ml_metrics": ml_collector.is_running,\n                "api_metrics": api_collector.is_running,\n                "performance_metrics": performance_collector.is_running,\n                "business_intelligence": bi_collector.is_running\n            }\n        }\n        \n        return JSONResponse(content=health_status)\n        \n    except Exception as e:\n        return JSONResponse(\n            status_code=500,\n            content={\n                "status": "unhealthy",\n                "error": str(e),\n                "timestamp": datetime.now(timezone.utc).isoformat()\n            }\n        )\n'
    with open(project_root / "examples" / "dashboard_setup.py", "w") as f:
        f.write(dashboard_setup_code)
    logger.info("Created dashboard setup example: examples/dashboard_setup.py")


def display_integration_summary():
    """Display summary of the integration process."""
    integration_summary = "\n    ═══════════════════════════════════════════════════════════════════════════════\n    BUSINESS METRICS INTEGRATION SUMMARY\n    ═══════════════════════════════════════════════════════════════════════════════\n    \n    🚀 INTEGRATION COMPLETED SUCCESSFULLY!\n    \n    📁 CREATED FILES:\n    ├── examples/fastapi_metrics_integration.py       # FastAPI middleware setup\n    ├── examples/database_metrics_integration.py      # Database operation tracking\n    ├── examples/ml_service_metrics_integration.py    # ML service instrumentation\n    ├── examples/cost_tracking_integration.py         # Cost tracking and monitoring\n    └── examples/dashboard_setup.py                   # Real-time dashboard endpoints\n    \n    🔧 INTEGRATION STEPS COMPLETED:\n    ✅ 1. Metrics collection system initialized\n    ✅ 2. Application components instrumented\n    ✅ 3. FastAPI middleware configured\n    ✅ 4. Database operations instrumented\n    ✅ 5. ML services instrumented\n    ✅ 6. Cost tracking implemented\n    ✅ 7. Dashboard endpoints created\n    \n    📊 METRICS BEING COLLECTED:\n    \n    🤖 ML-Specific Metrics:\n       • Prompt improvement success rates by category\n       • Model inference accuracy and confidence distributions\n       • Feature flag usage and rollout effectiveness\n       • ML pipeline processing times and throughput\n    \n    🌐 API Usage Metrics:\n       • Endpoint popularity and usage patterns\n       • User journey tracking and conversion rates\n       • Rate limiting effectiveness and user impact\n       • Authentication success/failure rates\n    \n    ⚡ Performance Metrics:\n       • Request processing pipeline stages\n       • Database query performance by operation type\n       • Cache effectiveness and hit ratios\n       • External API dependency performance\n    \n    💼 Business Intelligence Metrics:\n       • Feature adoption rates and user engagement\n       • Error patterns and user impact analysis\n       • Resource utilization efficiency\n       • Cost per operation tracking\n    \n    🔗 INTEGRATION INSTRUCTIONS:\n    \n    1. Add to your FastAPI main app:\n       ```python\n       from examples.fastapi_metrics_integration import app\n       # Use the configured app with metrics middleware\n       ```\n    \n    2. Wrap your database operations:\n       ```python\n       from examples.database_metrics_integration import MetricsEnabledDatabase\n       db = MetricsEnabledDatabase(your_connection_pool)\n       ```\n    \n    3. Instrument your ML services:\n       ```python\n       from examples.ml_service_metrics_integration import PromptImprovementService\n       service = PromptImprovementService()\n       ```\n    \n    4. Add cost tracking to operations:\n       ```python\n       from examples.cost_tracking_integration import ResourceUsageTracker\n       cost_tracker = ResourceUsageTracker()\n       ```\n    \n    5. Setup dashboard endpoints:\n       ```python\n       from examples.dashboard_setup import dashboard_router\n       app.include_router(dashboard_router)\n       ```\n    \n    📈 DASHBOARD ENDPOINTS AVAILABLE:\n    • GET /dashboard/executive-summary        # Executive-level KPIs\n    • GET /dashboard/ml-performance          # ML metrics and performance\n    • GET /dashboard/real-time-monitoring    # Live system status\n    • GET /dashboard/business-insights       # AI-powered insights\n    • GET /dashboard/feature-adoption        # Feature usage analytics\n    • GET /dashboard/cost-efficiency         # Cost analysis and optimization\n    • WebSocket /dashboard/real-time-updates # Live data streaming\n    \n    💡 NEXT STEPS:\n    1. Run the comprehensive demo: python demo_comprehensive_business_metrics.py\n    2. Integrate middleware into your FastAPI app\n    3. Configure dashboard access and authentication\n    4. Set up alerting thresholds for business metrics\n    5. Create custom visualizations using the exported data\n    \n    🔍 MONITORING & ALERTING:\n    • Real-time cost monitoring with configurable thresholds\n    • Performance degradation detection\n    • Feature adoption tracking and user engagement analysis\n    • Business KPI dashboards with export capabilities\n    \n    ═══════════════════════════════════════════════════════════════════════════════\n    "
    print(integration_summary)


async def main():
    """Main function to run the integration script."""
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )
    logger.info("Starting business metrics integration...")
    try:
        await integrate_business_metrics()
        logger.info("Integration completed successfully!")
    except Exception as e:
        logger.error("Integration failed: %s", e)
        raise


if __name__ == "__main__":
    asyncio.run(main())
