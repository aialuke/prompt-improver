#!/usr/bin/env python3
"""Performance validation script for the Unified Monitoring System.

This script validates that the monitoring consolidation meets SRE requirements:
- Response times <100ms for health checks
- No monitoring regressions
- All existing monitoring capabilities maintained
- Clean integration with health endpoints
"""

import asyncio
import json
import sys
import time
from pathlib import Path
from typing import Any
from unittest.mock import patch

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from prompt_improver.monitoring.unified import (
    ComponentCategory,
    HealthStatus,
    MetricType,
    UnifiedMonitoringFacade,
    create_monitoring_config,
)
from prompt_improver.monitoring.unified.types import (
    HealthCheckResult,
    MetricPoint,
    SystemHealthSummary,
)


class MonitoringValidator:
    """Validates the unified monitoring system performance and functionality."""

    def __init__(self) -> None:
        self.results: dict[str, Any] = {
            "performance_tests": {},
            "functionality_tests": {},
            "regression_tests": {},
            "overall_status": "pending",
        }

    async def run_all_validations(self) -> dict[str, Any]:
        """Run complete validation suite."""
        print("üîç Starting Unified Monitoring System Validation\n")

        # Run performance tests
        await self.validate_performance()

        # Run functionality tests
        await self.validate_functionality()

        # Run regression tests
        await self.validate_no_regressions()

        # Calculate overall status
        self._calculate_overall_status()

        return self.results

    async def validate_performance(self):
        """Validate performance requirements are met."""
        print("‚ö° Performance Validation")
        print("=" * 40)

        config = create_monitoring_config(
            health_check_timeout_seconds=10.0,
            parallel_enabled=True,
            max_concurrent_checks=5,
            metrics_enabled=True,
        )

        # Performance Test 1: Facade Initialization
        start_time = time.time()
        with patch('prompt_improver.monitoring.unified.facade.ManagerMode') as mock_mode:
            facade = UnifiedMonitoringFacade(config=config, manager_mode=mock_mode.HIGH_AVAILABILITY)
        initialization_time_ms = (time.time() - start_time) * 1000

        init_passed = initialization_time_ms < 100
        print(f"‚úì Facade initialization: {initialization_time_ms:.2f}ms {'(PASS)' if init_passed else '(FAIL)'}")

        self.results["performance_tests"]["initialization_time_ms"] = initialization_time_ms
        self.results["performance_tests"]["initialization_passed"] = init_passed

        # Performance Test 2: Health Check Speed
        with patch.object(facade.health_service, 'run_all_checks') as mock_checks:
            async def mock_health_check():
                await asyncio.sleep(0.02)  # Simulate 20ms health check
                return SystemHealthSummary(
                    overall_status=HealthStatus.HEALTHY,
                    total_components=4,
                    healthy_components=4,
                    degraded_components=0,
                    unhealthy_components=0,
                    unknown_components=0,
                    check_duration_ms=20.0,
                )
            mock_checks.side_effect = mock_health_check

            start_time = time.time()
            health_summary = await facade.get_system_health()
            health_check_time_ms = (time.time() - start_time) * 1000

            health_passed = health_check_time_ms < 100
            print(f"‚úì System health check: {health_check_time_ms:.2f}ms {'(PASS)' if health_passed else '(FAIL)'}")

            self.results["performance_tests"]["health_check_time_ms"] = health_check_time_ms
            self.results["performance_tests"]["health_check_passed"] = health_passed

        # Performance Test 3: Metrics Collection Speed
        with patch.object(facade.metrics_service, 'get_all_metrics') as mock_metrics:
            async def mock_collect_metrics():
                await asyncio.sleep(0.01)  # Simulate 10ms metrics collection
                return [
                    MetricPoint(name=f"test.metric.{i}", value=float(i), metric_type=MetricType.GAUGE)
                    for i in range(20)
                ]
            mock_metrics.side_effect = mock_collect_metrics

            start_time = time.time()
            metrics = await facade.collect_all_metrics()
            metrics_time_ms = (time.time() - start_time) * 1000

            metrics_passed = metrics_time_ms < 50
            print(f"‚úì Metrics collection: {metrics_time_ms:.2f}ms {'(PASS)' if metrics_passed else '(FAIL)'}")

            self.results["performance_tests"]["metrics_collection_time_ms"] = metrics_time_ms
            self.results["performance_tests"]["metrics_collection_passed"] = metrics_passed

        # Performance Test 4: Concurrent Operations
        with patch.object(facade.health_service, 'run_all_checks') as mock_concurrent_checks:
            async def mock_concurrent_health():
                await asyncio.sleep(0.015)  # 15ms
                return SystemHealthSummary(
                    overall_status=HealthStatus.HEALTHY,
                    total_components=1,
                    healthy_components=1,
                    degraded_components=0,
                    unhealthy_components=0,
                    unknown_components=0,
                )
            mock_concurrent_checks.side_effect = mock_concurrent_health

            tasks = [facade.get_system_health() for _ in range(5)]
            start_time = time.time()
            results = await asyncio.gather(*tasks)
            concurrent_time_ms = (time.time() - start_time) * 1000

            concurrent_passed = concurrent_time_ms < 100  # Should use caching
            print(f"‚úì Concurrent health checks (5x): {concurrent_time_ms:.2f}ms {'(PASS)' if concurrent_passed else '(FAIL)'}")

            self.results["performance_tests"]["concurrent_time_ms"] = concurrent_time_ms
            self.results["performance_tests"]["concurrent_passed"] = concurrent_passed

        print()

    async def validate_functionality(self):
        """Validate all functional requirements are met."""
        print("üîß Functionality Validation")
        print("=" * 40)

        config = create_monitoring_config()
        with patch('prompt_improver.monitoring.unified.facade.ManagerMode') as mock_mode:
            facade = UnifiedMonitoringFacade(config=config, manager_mode=mock_mode.HIGH_AVAILABILITY)

        # Test 1: System Health Check
        with patch.object(facade.health_service, 'run_all_checks') as mock_checks:
            mock_summary = SystemHealthSummary(
                overall_status=HealthStatus.HEALTHY,
                total_components=4,
                healthy_components=4,
                degraded_components=0,
                unhealthy_components=0,
                unknown_components=0,
                check_duration_ms=25.5,
                component_results={
                    "database": HealthCheckResult(
                        status=HealthStatus.HEALTHY,
                        component_name="database",
                        message="Database healthy",
                        response_time_ms=15.2,
                        category=ComponentCategory.DATABASE,
                    ),
                    "redis": HealthCheckResult(
                        status=HealthStatus.HEALTHY,
                        component_name="redis",
                        message="Redis healthy",
                        response_time_ms=8.1,
                        category=ComponentCategory.CACHE,
                    ),
                }
            )
            mock_checks.return_value = mock_summary

            result = await facade.get_system_health()
            system_health_passed = (
                result.overall_status == HealthStatus.HEALTHY and
                result.total_components == 4 and
                len(result.component_results) == 2
            )
            print(f"‚úì System health check functionality: {'(PASS)' if system_health_passed else '(FAIL)'}")

            self.results["functionality_tests"]["system_health_check"] = system_health_passed

        # Test 2: Component Health Check
        with patch.object(facade.health_service, 'run_component_check') as mock_component:
            mock_result = HealthCheckResult(
                status=HealthStatus.HEALTHY,
                component_name="database",
                message="Database connection successful",
                response_time_ms=12.3,
                category=ComponentCategory.DATABASE,
                details={"active_connections": 5}
            )
            mock_component.return_value = mock_result

            result = await facade.check_component_health("database")
            component_health_passed = (
                result.status == HealthStatus.HEALTHY and
                result.component_name == "database" and
                result.details["active_connections"] == 5
            )
            print(f"‚úì Component health check functionality: {'(PASS)' if component_health_passed else '(FAIL)'}")

            self.results["functionality_tests"]["component_health_check"] = component_health_passed

        # Test 3: Metrics Collection
        with patch.object(facade.metrics_service, 'get_all_metrics') as mock_metrics:
            mock_metrics_list = [
                MetricPoint(
                    name="system.cpu.usage_percent",
                    value=25.4,
                    metric_type=MetricType.GAUGE,
                    unit="percent",
                    description="CPU usage percentage",
                ),
                MetricPoint(
                    name="app.requests.count",
                    value=1547,
                    metric_type=MetricType.COUNTER,
                    unit="count",
                    description="Total request count",
                ),
            ]
            mock_metrics.return_value = mock_metrics_list

            metrics = await facade.collect_all_metrics()
            metrics_passed = (
                len(metrics) == 2 and
                metrics[0].name == "system.cpu.usage_percent" and
                metrics[1].name == "app.requests.count"
            )
            print(f"‚úì Metrics collection functionality: {'(PASS)' if metrics_passed else '(FAIL)'}")

            self.results["functionality_tests"]["metrics_collection"] = metrics_passed

        # Test 4: Custom Metrics Recording
        with patch.object(facade.metrics_service, 'record_metric') as mock_record:
            facade.record_custom_metric(
                "custom.api.response_time",
                45.7,
                tags={"endpoint": "/health"}
            )

            custom_metrics_passed = mock_record.called
            recorded_metric = mock_record.call_args[0][0] if mock_record.called else None
            if recorded_metric:
                custom_metrics_passed = (
                    recorded_metric.name == "custom.api.response_time" and
                    recorded_metric.value == 45.7 and
                    recorded_metric.tags["endpoint"] == "/health"
                )

            print(f"‚úì Custom metrics recording: {'(PASS)' if custom_metrics_passed else '(FAIL)'}")

            self.results["functionality_tests"]["custom_metrics"] = custom_metrics_passed

        # Test 5: Monitoring Summary
        with patch.object(facade, 'get_system_health') as mock_health, \
             patch.object(facade, 'collect_all_metrics') as mock_metrics:

            mock_health.return_value = SystemHealthSummary(
                overall_status=HealthStatus.HEALTHY,
                total_components=4,
                healthy_components=4,
                degraded_components=0,
                unhealthy_components=0,
                unknown_components=0,
            )

            mock_metrics.return_value = [
                MetricPoint(name="test.metric", value=1.0, metric_type=MetricType.GAUGE)
            ]

            summary = await facade.get_monitoring_summary()
            summary_passed = (
                "health" in summary and
                "metrics" in summary and
                "components" in summary and
                summary["health"]["overall_status"] == "healthy"
            )
            print(f"‚úì Monitoring summary: {'(PASS)' if summary_passed else '(FAIL)'}")

            self.results["functionality_tests"]["monitoring_summary"] = summary_passed

        print()

    async def validate_no_regressions(self):
        """Validate that no existing functionality has regressed."""
        print("üîÑ Regression Testing")
        print("=" * 40)

        # Test 1: Health Endpoint Structure
        from prompt_improver.api.health import health_router

        endpoint_paths = [route.path for route in health_router.routes if hasattr(route, 'path')]

        expected_endpoints = [
            "/health/liveness",
            "/health/live",
            "/health/readiness",
            "/health/ready",
            "/health/startup",
            "/health/",
            "/health/deep",
            "/health/component/{component_name}",
            "/health/metrics",
            "/health/summary",
            "/health/cleanup",
        ]

        missing_endpoints = []
        for expected in expected_endpoints:
            # Check for exact match or pattern match for parameterized routes
            found = any(
                path == expected or
                ('{' in expected and path.replace('{component_name}', 'test') == expected.replace('{component_name}', 'test'))
                for path in endpoint_paths
            )
            if not found:
                missing_endpoints.append(expected)

        endpoints_passed = len(missing_endpoints) == 0
        print(f"‚úì Health endpoints preserved: {'(PASS)' if endpoints_passed else '(FAIL)'}")
        if missing_endpoints:
            print(f"  Missing endpoints: {missing_endpoints}")

        self.results["regression_tests"]["health_endpoints"] = endpoints_passed

        # Test 2: Factory Functions Available
        try:
            config = create_monitoring_config()
            facade_factory_passed = config is not None
            print(f"‚úì Factory functions available: {'(PASS)' if facade_factory_passed else '(FAIL)'}")
        except Exception as e:
            facade_factory_passed = False
            print(f"‚úó Factory functions available: (FAIL) - {e}")

        self.results["regression_tests"]["factory_functions"] = facade_factory_passed

        # Test 3: All Health Checker Components Available
        from prompt_improver.monitoring.unified import (
            DatabaseHealthChecker,
            MLModelsHealthChecker,
            RedisHealthChecker,
            SystemResourcesHealthChecker,
        )

        health_checkers_available = True
        health_checker_classes = [
            DatabaseHealthChecker,
            RedisHealthChecker,
            MLModelsHealthChecker,
            SystemResourcesHealthChecker,
        ]

        for checker_class in health_checker_classes:
            try:
                checker = checker_class(timeout_seconds=5.0)
                has_methods = (
                    hasattr(checker, 'check_health') and
                    hasattr(checker, 'get_component_name') and
                    hasattr(checker, 'get_timeout_seconds')
                )
                if not has_methods:
                    health_checkers_available = False
                    break
            except Exception:
                health_checkers_available = False
                break

        print(f"‚úì Health checker components: {'(PASS)' if health_checkers_available else '(FAIL)'}")

        self.results["regression_tests"]["health_checkers"] = health_checkers_available

        # Test 4: Type System Completeness
        from prompt_improver.monitoring.unified.types import (
            ComponentCategory,
            HealthCheckResult,
            HealthStatus,
            MetricPoint,
            MetricType,
            MonitoringConfig,
            SystemHealthSummary,
        )

        types_complete = True
        required_enums = [HealthStatus, MetricType, ComponentCategory]
        required_classes = [HealthCheckResult, MetricPoint, SystemHealthSummary, MonitoringConfig]

        for enum_class in required_enums:
            if not hasattr(enum_class, '_member_names_'):
                types_complete = False
                break

        for data_class in required_classes:
            if not hasattr(data_class, '__dataclass_fields__'):
                types_complete = False
                break

        print(f"‚úì Type system completeness: {'(PASS)' if types_complete else '(FAIL)'}")

        self.results["regression_tests"]["type_system"] = types_complete

        print()

    def _calculate_overall_status(self) -> None:
        """Calculate overall validation status."""
        all_tests = []

        # Collect all test results
        for category in ["performance_tests", "functionality_tests", "regression_tests"]:
            all_tests.extend(result for result in self.results[category].values() if isinstance(result, bool))

        if not all_tests:
            self.results["overall_status"] = "no_tests"
        elif all(all_tests):
            self.results["overall_status"] = "passed"
        else:
            self.results["overall_status"] = "failed"

        passed_count = sum(all_tests)
        total_count = len(all_tests)
        self.results["test_summary"] = {
            "passed": passed_count,
            "total": total_count,
            "pass_rate": (passed_count / total_count * 100) if total_count > 0 else 0,
        }

    def print_summary(self):
        """Print validation summary."""
        print("üìä VALIDATION SUMMARY")
        print("=" * 50)

        summary = self.results["test_summary"]
        status = self.results["overall_status"]

        print(f"Overall Status: {status.upper()}")
        print(f"Tests Passed: {summary['passed']}/{summary['total']} ({summary['pass_rate']:.1f}%)")

        if status == "passed":
            print("\n‚úÖ All monitoring consolidation requirements PASSED!")
            print("   - Performance requirements met (<100ms health checks)")
            print("   - All functionality preserved")
            print("   - No regressions detected")
            print("   - UnifiedMonitoringFacade ready for production")
        else:
            print(f"\n‚ùå Validation FAILED - {summary['total'] - summary['passed']} tests failed")
            print("   Review failed tests above and address issues before deployment")

        print()


async def main():
    """Main validation entry point."""
    validator = MonitoringValidator()

    try:
        results = await validator.run_all_validations()
        validator.print_summary()

        # Save results to file for CI/CD integration
        results_file = Path(__file__).parent.parent / "monitoring_validation_results.json"
        with open(results_file, 'w', encoding="utf-8") as f:
            json.dump(results, f, indent=2, default=str)

        print(f"üìÅ Detailed results saved to: {results_file}")

        # Exit with appropriate code for CI/CD
        if results["overall_status"] == "passed":
            sys.exit(0)
        else:
            sys.exit(1)

    except Exception as e:
        print(f"‚ùå Validation failed with error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
