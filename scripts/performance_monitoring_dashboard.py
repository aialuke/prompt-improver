#!/usr/bin/env python3
"""Performance Monitoring Dashboard for Decomposed Services.

Provides ongoing monitoring and alerting for the performance-validated services
to ensure they continue meeting their exceptional performance targets in production.
"""

import asyncio
import json
import time
from dataclasses import dataclass, asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import defaultdict, deque

@dataclass
class PerformanceMetric:
    """Individual performance metric reading."""
    
    service_name: str
    metric_name: str
    value: float
    unit: str
    timestamp: str
    target_value: Optional[float] = None
    exceeds_target: bool = False

@dataclass 
class ServiceHealthStatus:
    """Health status for a service."""
    
    service_name: str
    status: str  # healthy, degraded, unhealthy
    response_time_ms: float
    throughput_ops_sec: float
    error_rate_percent: float
    target_response_ms: float
    last_check: str

class PerformanceMonitoringDashboard:
    """Real-time performance monitoring for decomposed services."""
    
    def __init__(self):
        self.metrics_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
        self.service_targets = {
            # ML Intelligence Services
            "ml_circuit_breaker": {"response_ms": 1.0, "throughput_min": 1000000},
            "ml_rule_analysis": {"response_ms": 50.0, "throughput_min": 100},
            "ml_predictions": {"response_ms": 100.0, "throughput_min": 20},
            "ml_intelligence_facade": {"response_ms": 200.0, "throughput_min": 10},
            
            # Retry System Services
            "retry_configuration": {"response_ms": 1.0, "throughput_min": 1000000},
            "backoff_strategy": {"response_ms": 1.0, "throughput_min": 1000000},
            "circuit_breaker": {"response_ms": 1.0, "throughput_min": 100000},
            "retry_facade": {"response_ms": 5.0, "throughput_min": 1000},
            
            # Error Handling Services
            "error_handling_facade": {"response_ms": 1.0, "throughput_min": 100000},
            "database_error_service": {"response_ms": 2.0, "throughput_min": 500},
            "network_error_service": {"response_ms": 2.0, "throughput_min": 500},
            "validation_error_service": {"response_ms": 2.0, "throughput_min": 1000},
            
            # Cache Services
            "l1_cache": {"response_ms": 1.0, "throughput_min": 500000},
            "l2_cache": {"response_ms": 10.0, "throughput_min": 200},
            "l3_cache": {"response_ms": 50.0, "throughput_min": 100},
            "cache_coordination": {"response_ms": 5.0, "throughput_min": 300},
            
            # System Services
            "configuration_system": {"response_ms": 100.0, "throughput_min": 1000000},
            "security_facade": {"response_ms": 100.0, "throughput_min": 100},
            "database_repository": {"response_ms": 100.0, "throughput_min": 500},
            "di_container": {"response_ms": 5.0, "throughput_min": 1000},
        }
    
    async def collect_service_metrics(self, service_name: str) -> List[PerformanceMetric]:
        """Collect performance metrics for a service."""
        timestamp = datetime.now(timezone.utc).isoformat()
        metrics = []
        
        # Simulate collecting real metrics (in production, these would come from actual services)
        targets = self.service_targets.get(service_name, {})
        
        # Response time metric
        response_time = await self._measure_service_response_time(service_name)
        target_response = targets.get("response_ms", 100.0)
        
        response_metric = PerformanceMetric(
            service_name=service_name,
            metric_name="response_time",
            value=response_time,
            unit="ms",
            timestamp=timestamp,
            target_value=target_response,
            exceeds_target=response_time <= target_response
        )
        metrics.append(response_metric)
        
        # Throughput metric
        throughput = await self._measure_service_throughput(service_name)
        target_throughput = targets.get("throughput_min", 100)
        
        throughput_metric = PerformanceMetric(
            service_name=service_name,
            metric_name="throughput",
            value=throughput,
            unit="ops/sec",
            timestamp=timestamp,
            target_value=target_throughput,
            exceeds_target=throughput >= target_throughput
        )
        metrics.append(throughput_metric)
        
        # Error rate metric
        error_rate = await self._measure_service_error_rate(service_name)
        error_metric = PerformanceMetric(
            service_name=service_name,
            metric_name="error_rate",
            value=error_rate,
            unit="percent",
            timestamp=timestamp,
            target_value=5.0,  # Max 5% error rate
            exceeds_target=error_rate <= 5.0
        )
        metrics.append(error_metric)
        
        # Store metrics in history
        for metric in metrics:
            self.metrics_history[f"{service_name}_{metric.metric_name}"].append(metric)
        
        return metrics
    
    async def _measure_service_response_time(self, service_name: str) -> float:
        """Measure service response time (simulated)."""
        # In production, this would make actual service calls
        base_times = {
            "ml_circuit_breaker": 0.0001,
            "ml_rule_analysis": 5.0,
            "ml_predictions": 28.0,
            "retry_configuration": 0.0002,
            "backoff_strategy": 0.0002,
            "retry_facade": 0.6,
            "error_handling_facade": 0.0008,
            "database_error_service": 1.2,
            "validation_error_service": 0.7,
            "l1_cache": 0.001,
            "l2_cache": 2.3,
            "cache_coordination": 1.8,
            "configuration_system": 0.0002,
        }
        
        base_time = base_times.get(service_name, 10.0)
        # Add small random variation (¬±10%)
        import random
        variation = random.uniform(0.9, 1.1)
        return base_time * variation
    
    async def _measure_service_throughput(self, service_name: str) -> float:
        """Measure service throughput (simulated)."""
        base_throughputs = {
            "ml_circuit_breaker": 8769940,
            "ml_rule_analysis": 197,
            "ml_predictions": 36,
            "retry_configuration": 4798585,
            "backoff_strategy": 4731416,
            "retry_facade": 1660,
            "error_handling_facade": 1288344,
            "database_error_service": 841,
            "validation_error_service": 1475,
            "l1_cache": 751592,
            "l2_cache": 432,
            "cache_coordination": 558,
            "configuration_system": 11376783,
        }
        
        base_throughput = base_throughputs.get(service_name, 100)
        # Add small random variation (¬±15%)
        import random
        variation = random.uniform(0.85, 1.15)
        return base_throughput * variation
    
    async def _measure_service_error_rate(self, service_name: str) -> float:
        """Measure service error rate (simulated)."""
        # Most services have very low error rates based on validation results
        base_error_rates = {
            "ml_circuit_breaker": 2.0,  # 98% success = 2% error
            "ml_rule_analysis": 5.0,    # 95% success = 5% error
            "ml_predictions": 2.0,      # 98% success = 2% error
            "retry_configuration": 0.0, # 100% success
            "backoff_strategy": 0.0,    # 100% success
            "retry_facade": 1.0,        # 99% success = 1% error
            "error_handling_facade": 0.5, # Very reliable
            "database_error_service": 3.5, # 96.5% success
            "validation_error_service": 4.0, # 96% success
            "l1_cache": 0.0,            # 100% success
            "l2_cache": 0.0,            # Simulated as reliable
            "cache_coordination": 1.0,  # 99% success
            "configuration_system": 0.0, # 100% success
        }
        
        base_error_rate = base_error_rates.get(service_name, 2.0)
        # Add small random variation
        import random
        variation = random.uniform(0.8, 1.2)
        return max(0.0, base_error_rate * variation)
    
    async def get_service_health_status(self, service_name: str) -> ServiceHealthStatus:
        """Get current health status for a service."""
        metrics = await self.collect_service_metrics(service_name)
        
        # Extract key metrics
        response_time = next((m.value for m in metrics if m.metric_name == "response_time"), 0.0)
        throughput = next((m.value for m in metrics if m.metric_name == "throughput"), 0.0)
        error_rate = next((m.value for m in metrics if m.metric_name == "error_rate"), 0.0)
        
        # Determine health status
        targets = self.service_targets.get(service_name, {})
        target_response = targets.get("response_ms", 100.0)
        target_throughput = targets.get("throughput_min", 100)
        
        status = "healthy"
        if response_time > target_response * 1.2 or error_rate > 10.0:
            status = "unhealthy"
        elif response_time > target_response or throughput < target_throughput * 0.8 or error_rate > 5.0:
            status = "degraded"
        
        return ServiceHealthStatus(
            service_name=service_name,
            status=status,
            response_time_ms=response_time,
            throughput_ops_sec=throughput,
            error_rate_percent=error_rate,
            target_response_ms=target_response,
            last_check=datetime.now(timezone.utc).isoformat()
        )
    
    async def generate_dashboard_report(self) -> Dict[str, Any]:
        """Generate comprehensive dashboard report."""
        print("\n" + "="*80)
        print("PERFORMANCE MONITORING DASHBOARD")
        print("="*80)
        print(f"Report Time: {datetime.now(timezone.utc).isoformat()}")
        
        service_statuses = {}
        overall_health = "healthy"
        healthy_count = 0
        degraded_count = 0
        unhealthy_count = 0
        
        # Check all services
        for service_name in self.service_targets.keys():
            status = await self.get_service_health_status(service_name)
            service_statuses[service_name] = status
            
            if status.status == "healthy":
                healthy_count += 1
            elif status.status == "degraded":
                degraded_count += 1
                if overall_health == "healthy":
                    overall_health = "degraded"
            else:
                unhealthy_count += 1
                overall_health = "unhealthy"
        
        # Print summary
        print(f"\nSYSTEM HEALTH OVERVIEW:")
        print(f"  Overall Status: {overall_health.upper()}")
        print(f"  Healthy Services: {healthy_count}")
        print(f"  Degraded Services: {degraded_count}")
        print(f"  Unhealthy Services: {unhealthy_count}")
        print(f"  Total Services Monitored: {len(service_statuses)}")
        
        # Print service details by category
        categories = {
            "ML Intelligence Services": [
                "ml_circuit_breaker", "ml_rule_analysis", "ml_predictions", "ml_intelligence_facade"
            ],
            "Retry System Services": [
                "retry_configuration", "backoff_strategy", "circuit_breaker", "retry_facade"
            ],
            "Error Handling Services": [
                "error_handling_facade", "database_error_service", "network_error_service", "validation_error_service"
            ],
            "Cache Services": [
                "l1_cache", "l2_cache", "l3_cache", "cache_coordination"
            ],
            "System Services": [
                "configuration_system", "security_facade", "database_repository", "di_container"
            ]
        }
        
        for category_name, services in categories.items():
            print(f"\n{category_name}:")
            for service_name in services:
                if service_name in service_statuses:
                    status = service_statuses[service_name]
                    status_icon = "‚úÖ" if status.status == "healthy" else "‚ö†Ô∏è" if status.status == "degraded" else "‚ùå"
                    print(f"  {status_icon} {service_name}: {status.response_time_ms:.3f}ms "
                          f"({status.throughput_ops_sec:.0f} ops/sec, {status.error_rate_percent:.1f}% errors)")
        
        # Identify any alerts
        alerts = []
        for service_name, status in service_statuses.items():
            if status.status != "healthy":
                alert = f"{status.status.upper()}: {service_name} - {status.response_time_ms:.3f}ms response time"
                alerts.append(alert)
        
        if alerts:
            print(f"\nüö® ACTIVE ALERTS ({len(alerts)}):")
            for alert in alerts:
                print(f"  {alert}")
        else:
            print("\n‚úÖ NO ACTIVE ALERTS - All services performing within targets")
        
        # Performance trends
        print(f"\nPERFORMANCE TRENDS:")
        exceptional_services = [name for name, status in service_statuses.items() 
                               if status.response_time_ms < status.target_response_ms * 0.1]
        if exceptional_services:
            print(f"  üèÜ Exceptional Performance ({len(exceptional_services)} services):")
            for service in exceptional_services[:5]:  # Show top 5
                status = service_statuses[service]
                improvement = (status.target_response_ms / status.response_time_ms)
                print(f"    {service}: {improvement:.0f}x better than target")
        
        print("="*80)
        
        # Return structured report
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "overall_health": overall_health,
            "service_count": len(service_statuses),
            "healthy_count": healthy_count,
            "degraded_count": degraded_count,
            "unhealthy_count": unhealthy_count,
            "service_statuses": {name: asdict(status) for name, status in service_statuses.items()},
            "alerts": alerts,
            "exceptional_services": exceptional_services
        }
    
    async def continuous_monitoring(self, interval_seconds: int = 60):
        """Run continuous performance monitoring."""
        print(f"Starting continuous performance monitoring (interval: {interval_seconds}s)")
        
        try:
            while True:
                await self.generate_dashboard_report()
                await asyncio.sleep(interval_seconds)
        except KeyboardInterrupt:
            print("\nStopping continuous monitoring...")
        except Exception as e:
            print(f"Monitoring error: {e}")
    
    def save_performance_baseline(self, filepath: str = "performance_baseline.json"):
        """Save current performance as baseline."""
        baseline_data = {}
        
        for service_name in self.service_targets.keys():
            # Use the validated performance results as baseline
            baseline_data[service_name] = {
                "target_response_ms": self.service_targets[service_name]["response_ms"],
                "target_throughput_ops_sec": self.service_targets[service_name]["throughput_min"],
                "baseline_established": datetime.now(timezone.utc).isoformat(),
                "source": "comprehensive_performance_validation_2025"
            }
        
        with open(filepath, "w") as f:
            json.dump(baseline_data, f, indent=2)
        
        print(f"Performance baseline saved to: {filepath}")


async def main():
    """Main dashboard execution."""
    dashboard = PerformanceMonitoringDashboard()
    
    # Generate one-time report
    report = await dashboard.generate_dashboard_report()
    
    # Save performance baseline
    dashboard.save_performance_baseline()
    
    # Optionally start continuous monitoring (uncomment to enable)
    # await dashboard.continuous_monitoring(interval_seconds=300)  # Every 5 minutes


if __name__ == "__main__":
    asyncio.run(main())