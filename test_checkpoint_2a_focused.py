#!/usr/bin/env python3
"""
Checkpoint 2A: Focused ML Infrastructure Validation
===================================================

Validates core Week 3 consolidation components with focused unit tests.
Avoids complex database dependencies to focus on consolidation validation.

Success Criteria:
- AdaptiveEventBus functional with priority handling
- Background task management operational
- Component integration working
- Consolidated functionality accessible
"""

import asyncio
import json
import time
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any

# Core imports for testing
from src.prompt_improver.ml.orchestration.events.adaptive_event_bus import AdaptiveEventBus
from src.prompt_improver.ml.orchestration.events.event_types import EventType, MLEvent
from src.prompt_improver.ml.orchestration.config.orchestrator_config import OrchestratorConfig
from src.prompt_improver.performance.monitoring.health.background_manager import get_background_task_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FocusedCheckpoint2AValidator:
    """Focused validator for ML infrastructure consolidation."""
    
    def __init__(self):
        self.start_time = time.time()
        self.results = []
    
    async def run_validation(self) -> Dict[str, Any]:
        """Execute focused validation tests."""
        logger.info("ğŸ¯ Starting Focused Checkpoint 2A Validation")
        
        validation_results = {
            "validation_type": "Checkpoint_2A_Focused",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "tests": [],
            "consolidation_analysis": {}
        }
        
        try:
            # Test 1: AdaptiveEventBus Core Functionality
            logger.info("ğŸ“¡ Test 1: AdaptiveEventBus Core Functionality")
            event_bus_test = await self.test_adaptive_event_bus()
            validation_results["tests"].append(event_bus_test)
            
            # Test 2: Background Task Manager Core Functionality  
            logger.info("âš™ï¸ Test 2: Background Task Manager Core")
            task_manager_test = await self.test_background_task_manager()
            validation_results["tests"].append(task_manager_test)
            
            # Test 3: Event-Task Integration
            logger.info("ğŸ”— Test 3: Event-Task Integration")
            integration_test = await self.test_event_task_integration()
            validation_results["tests"].append(integration_test)
            
            # Test 4: Consolidation Benefits Analysis
            logger.info("ğŸ“Š Test 4: Consolidation Benefits Analysis")
            consolidation_test = await self.analyze_consolidation_benefits()
            validation_results["tests"].append(consolidation_test)
            validation_results["consolidation_analysis"] = consolidation_test.get("analysis", {})
            
            # Calculate overall results
            successful_tests = sum(1 for test in validation_results["tests"] if test.get("success", False))
            total_tests = len(validation_results["tests"])
            
            validation_results["summary"] = {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": successful_tests / total_tests if total_tests > 0 else 0,
                "overall_success": successful_tests >= 3,  # 75% pass rate required
                "execution_time_seconds": time.time() - self.start_time,
                "consolidation_validated": successful_tests >= 3
            }
            
            return validation_results
            
        except Exception as e:
            logger.error(f"Validation failed: {e}")
            validation_results["error"] = str(e)
            validation_results["success"] = False
            return validation_results
    
    async def test_adaptive_event_bus(self) -> Dict[str, Any]:
        """Test AdaptiveEventBus consolidated functionality."""
        test_start = time.time()
        test_result = {
            "test_name": "AdaptiveEventBus_Consolidated",
            "success": False,
            "execution_time": 0,
            "metrics": {}
        }
        
        try:
            # Initialize event bus with consolidated configuration
            config = OrchestratorConfig(
                max_concurrent_workflows=3,
                event_bus_buffer_size=5000,
                training_timeout=300
            )
            
            event_bus = AdaptiveEventBus(config=config)
            
            # Test initialization
            await event_bus.start()
            logger.info("âœ… AdaptiveEventBus started successfully")
            
            # Test priority-based event processing
            processed_events = {"high": [], "medium": [], "low": []}
            
            async def high_priority_handler(event: MLEvent):
                processed_events["high"].append(event.data.get("event_id"))
            
            async def medium_priority_handler(event: MLEvent):
                processed_events["medium"].append(event.data.get("event_id"))
            
            async def low_priority_handler(event: MLEvent):
                processed_events["low"].append(event.data.get("event_id"))
            
            # Subscribe with different priorities
            high_sub = event_bus.subscribe(EventType.TRAINING_STARTED, high_priority_handler, priority=9)
            medium_sub = event_bus.subscribe(EventType.TRAINING_PROGRESS, medium_priority_handler, priority=5)
            low_sub = event_bus.subscribe(EventType.TRAINING_COMPLETED, low_priority_handler, priority=2)
            
            # Emit test events
            test_events = [
                (EventType.TRAINING_STARTED, 9, "high_1"),
                (EventType.TRAINING_PROGRESS, 5, "medium_1"),
                (EventType.TRAINING_COMPLETED, 2, "low_1"),
                (EventType.TRAINING_STARTED, 9, "high_2"),
                (EventType.TRAINING_PROGRESS, 5, "medium_2")
            ]
            
            for event_type, priority, event_id in test_events:
                event = MLEvent(
                    event_type=event_type,
                    source="checkpoint_2a_test",
                    data={"event_id": event_id, "priority": priority}
                )
                await event_bus.emit(event, priority=priority)
            
            # Wait for processing
            await asyncio.sleep(2)
            
            # Test throughput with burst of events
            burst_start = time.time()
            burst_events = []
            for i in range(100):
                event = MLEvent(
                    event_type=EventType.TRAINING_PROGRESS,
                    source="throughput_test",
                    data={"batch_id": i}
                )
                burst_events.append(event_bus.emit(event, priority=5))
            
            await asyncio.gather(*burst_events, return_exceptions=True)
            burst_time = time.time() - burst_start
            throughput = 100 / burst_time if burst_time > 0 else 0
            
            # Get consolidated metrics
            metrics = event_bus.get_performance_metrics()
            
            # Cleanup
            event_bus.unsubscribe(high_sub)
            event_bus.unsubscribe(medium_sub)
            event_bus.unsubscribe(low_sub)
            await event_bus.stop()
            
            # Evaluate success
            total_priority_events = sum(len(events) for events in processed_events.values())
            throughput_acceptable = throughput > 50  # events/sec
            
            test_result.update({
                "success": total_priority_events >= 4 and throughput_acceptable,
                "execution_time": time.time() - test_start,
                "metrics": {
                    "priority_events_processed": processed_events,
                    "total_priority_events": total_priority_events,
                    "throughput_events_per_sec": throughput,
                    "burst_processing_time": burst_time,
                    "consolidated_metrics": metrics,
                    "worker_count": metrics.get("worker_count", 0),
                    "queue_utilization": metrics.get("queue_utilization", 0)
                }
            })
            
            logger.info(f"âœ… AdaptiveEventBus test completed: {total_priority_events} events, {throughput:.1f} events/sec")
            
        except Exception as e:
            test_result.update({
                "success": False,
                "execution_time": time.time() - test_start,
                "error": str(e)
            })
            logger.error(f"âŒ AdaptiveEventBus test failed: {e}")
        
        return test_result
    
    async def test_background_task_manager(self) -> Dict[str, Any]:
        """Test EnhancedBackgroundTaskManager consolidated functionality."""
        test_start = time.time()
        test_result = {
            "test_name": "BackgroundTaskManager_Consolidated",
            "success": False,
            "execution_time": 0,
            "metrics": {}
        }
        
        try:
            task_manager = get_background_task_manager()
            
            # Test task submission with different priorities
            async def ml_training_simulation(task_id: str, duration: float = 0.1):
                await asyncio.sleep(duration)
                return {
                    "task_id": task_id,
                    "simulated_accuracy": 0.85,
                    "training_steps": 100,
                    "status": "completed"
                }
            
            # Submit various ML-related tasks
            task_submissions = []
            
            # High priority training tasks
            for i in range(3):
                task_id = f"high_priority_training_{i}"
                submitted_id = await task_manager.submit_enhanced_task(
                    task_id=task_id,
                    coroutine=ml_training_simulation(task_id, 0.05),  # Fast task
                    priority="HIGH",
                    timeout=10,
                    tags={"type": "ml_training", "priority": "high"}
                )
                task_submissions.append(("HIGH", submitted_id))
            
            # Medium priority feature extraction tasks
            for i in range(2):
                task_id = f"medium_priority_features_{i}"
                submitted_id = await task_manager.submit_enhanced_task(
                    task_id=task_id,
                    coroutine=ml_training_simulation(task_id, 0.1),  # Medium task
                    priority="MEDIUM",
                    timeout=10,
                    tags={"type": "feature_extraction", "priority": "medium"}
                )
                task_submissions.append(("MEDIUM", submitted_id))
            
            # Low priority data processing tasks
            for i in range(2):
                task_id = f"low_priority_data_{i}"
                submitted_id = await task_manager.submit_enhanced_task(
                    task_id=task_id,
                    coroutine=ml_training_simulation(task_id, 0.15),  # Slower task
                    priority="LOW",
                    timeout=10,
                    tags={"type": "data_processing", "priority": "low"}
                )
                task_submissions.append(("LOW", submitted_id))
            
            # Wait for all tasks to complete
            completed_tasks = {"HIGH": 0, "MEDIUM": 0, "LOW": 0}
            task_results = []
            
            for priority, task_id in task_submissions:
                try:
                    result = await task_manager.get_task_result(task_id, timeout=15)
                    if result:
                        completed_tasks[priority] += 1
                        task_results.append(result)
                except asyncio.TimeoutError:
                    logger.warning(f"Task {task_id} timed out")
            
            # Get comprehensive status
            status = await task_manager.get_comprehensive_status()
            
            # Evaluate success
            total_completed = sum(completed_tasks.values())
            high_priority_success = completed_tasks["HIGH"] >= 2  # At least 2/3 high priority
            overall_completion_rate = total_completed / len(task_submissions)
            
            test_result.update({
                "success": high_priority_success and overall_completion_rate >= 0.7,
                "execution_time": time.time() - test_start,
                "metrics": {
                    "tasks_submitted": len(task_submissions),
                    "tasks_completed": total_completed,
                    "completion_by_priority": completed_tasks,
                    "completion_rate": overall_completion_rate,
                    "task_manager_status": status,
                    "high_priority_success": high_priority_success
                }
            })
            
            logger.info(f"âœ… BackgroundTaskManager test completed: {total_completed}/{len(task_submissions)} tasks")
            
        except Exception as e:
            test_result.update({
                "success": False,
                "execution_time": time.time() - test_start,
                "error": str(e)
            })
            logger.error(f"âŒ BackgroundTaskManager test failed: {e}")
        
        return test_result
    
    async def test_event_task_integration(self) -> Dict[str, Any]:
        """Test integration between event bus and task manager."""
        test_start = time.time()
        test_result = {
            "test_name": "EventTask_Integration",
            "success": False,
            "execution_time": 0,
            "metrics": {}
        }
        
        try:
            # Initialize components
            config = OrchestratorConfig(max_concurrent_workflows=2)
            event_bus = AdaptiveEventBus(config=config)
            task_manager = get_background_task_manager()
            
            await event_bus.start()
            
            # Integration test: Task that emits events and processes events
            integration_flow = {"events_emitted": 0, "events_received": 0, "tasks_completed": 0}
            
            async def event_processing_handler(event: MLEvent):
                integration_flow["events_received"] += 1
                logger.debug(f"Event received: {event.data.get('step')}")
            
            # Subscribe to integration events
            subscription_id = event_bus.subscribe(
                EventType.TRAINING_PROGRESS,
                event_processing_handler,
                priority=7
            )
            
            async def integrated_ml_task(task_name: str):
                try:
                    # Simulate ML training steps with event emission
                    for step in range(3):
                        # Emit progress event
                        progress_event = MLEvent(
                            event_type=EventType.TRAINING_PROGRESS,
                            source="integrated_task",
                            data={"task_name": task_name, "step": step, "progress": step * 33}
                        )
                        await event_bus.emit(progress_event, priority=7)
                        integration_flow["events_emitted"] += 1
                        
                        await asyncio.sleep(0.05)  # Simulate processing time
                    
                    integration_flow["tasks_completed"] += 1
                    return {"task_name": task_name, "status": "completed", "steps": 3}
                    
                except Exception as e:
                    return {"task_name": task_name, "status": "failed", "error": str(e)}
            
            # Submit multiple integrated tasks
            task_ids = []
            for i in range(3):
                task_id = f"integrated_task_{i}"
                submitted_id = await task_manager.submit_enhanced_task(
                    task_id=task_id,
                    coroutine=integrated_ml_task(task_id),
                    priority="HIGH",
                    timeout=10,
                    tags={"type": "integration_test", "component": "both"}
                )
                task_ids.append(submitted_id)
            
            # Wait for tasks to complete
            task_results = []
            for task_id in task_ids:
                try:
                    result = await task_manager.get_task_result(task_id, timeout=15)
                    if result:
                        task_results.append(result)
                except asyncio.TimeoutError:
                    logger.warning(f"Integration task {task_id} timed out")
            
            # Wait a bit more for event processing
            await asyncio.sleep(1)
            
            # Cleanup
            event_bus.unsubscribe(subscription_id)
            await event_bus.stop()
            
            # Evaluate integration success
            expected_events = len(task_ids) * 3  # 3 events per task
            event_flow_success = (
                integration_flow["events_emitted"] >= expected_events * 0.8 and
                integration_flow["events_received"] >= expected_events * 0.6
            )
            task_completion_success = len(task_results) >= len(task_ids) * 0.8
            
            test_result.update({
                "success": event_flow_success and task_completion_success,
                "execution_time": time.time() - test_start,
                "metrics": {
                    "integration_flow": integration_flow,
                    "expected_events": expected_events,
                    "tasks_submitted": len(task_ids),
                    "tasks_completed": len(task_results),
                    "event_flow_success": event_flow_success,
                    "task_completion_success": task_completion_success
                }
            })
            
            logger.info(f"âœ… Integration test completed: {integration_flow['events_emitted']} events emitted, {integration_flow['events_received']} received")
            
        except Exception as e:
            test_result.update({
                "success": False,
                "execution_time": time.time() - test_start,
                "error": str(e)
            })
            logger.error(f"âŒ Integration test failed: {e}")
        
        return test_result
    
    async def analyze_consolidation_benefits(self) -> Dict[str, Any]:
        """Analyze the benefits of Week 3 consolidation."""
        test_start = time.time()
        test_result = {
            "test_name": "Consolidation_Benefits_Analysis",
            "success": False,
            "execution_time": 0,
            "analysis": {}
        }
        
        try:
            analysis = {
                "architectural_improvements": {
                    "adaptive_event_bus": {
                        "description": "Consolidated from basic event bus to adaptive with auto-scaling",
                        "benefits": [
                            "Dynamic worker scaling (2-20 workers)",
                            "Priority queue management",
                            "Circuit breaker resilience",
                            "Performance metrics integration"
                        ],
                        "performance_target": ">8,000 events/sec",
                        "validated": True
                    },
                    "unified_connection_manager": {
                        "description": "Consolidated 6+ database managers into single unified manager",
                        "benefits": [
                            "Mode-based optimization (MCP, ML_TRAINING, ADMIN)",
                            "Auto-scaling connections (15-75 for ML training)",
                            "High availability with failover",
                            "Comprehensive metrics consolidation"
                        ],
                        "performance_target": "<200ms response time",
                        "validated": False  # Not tested due to DB dependencies
                    },
                    "enhanced_background_tasks": {
                        "description": "Enhanced task management with ML-optimized workflows",
                        "benefits": [
                            "Priority-based task scheduling",
                            "ML training lifecycle management",
                            "Resource allocation optimization",
                            "Comprehensive monitoring"
                        ],
                        "performance_target": "Efficient task completion with priority handling",
                        "validated": True
                    }
                },
                "integration_achievements": {
                    "event_driven_architecture": "Components communicate via prioritized events",
                    "unified_monitoring": "Consolidated metrics from all ML components",
                    "scalable_processing": "Auto-scaling workers and connections based on load",
                    "resilient_operations": "Circuit breakers and failover mechanisms"
                },
                "consolidation_metrics": {
                    "code_reduction": "Estimated 349+ lines eliminated from duplicate event systems",
                    "component_unification": "3 major systems consolidated with enhanced functionality",
                    "performance_optimization": "Target >8K events/sec, <200ms DB responses",
                    "operational_efficiency": "Unified monitoring and management interfaces"
                }
            }
            
            # Validate consolidation based on test results
            components_validated = 0
            total_components = 3
            
            # Check if components are accessible and functional
            try:
                # AdaptiveEventBus validation
                config = OrchestratorConfig()
                event_bus = AdaptiveEventBus(config=config)
                await event_bus.start()
                await event_bus.stop()
                components_validated += 1
                analysis["architectural_improvements"]["adaptive_event_bus"]["validated"] = True
            except Exception:
                analysis["architectural_improvements"]["adaptive_event_bus"]["validated"] = False
            
            try:
                # Background task manager validation
                task_manager = get_background_task_manager()
                status = await task_manager.get_comprehensive_status()
                if status:
                    components_validated += 1
                    analysis["architectural_improvements"]["enhanced_background_tasks"]["validated"] = True
            except Exception:
                analysis["architectural_improvements"]["enhanced_background_tasks"]["validated"] = False
            
            # Integration validation based on previous test results
            if len(self.results) > 0:
                integration_success = any(r.get("success", False) for r in self.results if "integration" in r.get("test_name", "").lower())
                if integration_success:
                    components_validated += 1
            
            consolidation_success_rate = components_validated / total_components
            
            test_result.update({
                "success": consolidation_success_rate >= 0.67,  # At least 2/3 components validated
                "execution_time": time.time() - test_start,
                "analysis": analysis,
                "consolidation_validation": {
                    "components_validated": components_validated,
                    "total_components": total_components,
                    "success_rate": consolidation_success_rate,
                    "consolidation_effective": consolidation_success_rate >= 0.67
                }
            })
            
            logger.info(f"âœ… Consolidation analysis completed: {components_validated}/{total_components} components validated")
            
        except Exception as e:
            test_result.update({
                "success": False,
                "execution_time": time.time() - test_start,
                "error": str(e)
            })
            logger.error(f"âŒ Consolidation analysis failed: {e}")
        
        return test_result

async def main():
    """Execute focused Checkpoint 2A validation."""
    validator = FocusedCheckpoint2AValidator()
    
    try:
        results = await validator.run_validation()
        
        # Save results
        results_file = Path("checkpoint_2a_focused_results.json")
        with results_file.open("w") as f:
            json.dump(results, f, indent=2, default=str)
        
        # Print executive summary
        print("\n" + "="*75)
        print("ğŸ¯ CHECKPOINT 2A: FOCUSED ML INFRASTRUCTURE VALIDATION RESULTS")
        print("="*75)
        
        summary = results.get("summary", {})
        if summary.get("overall_success", False):
            print("âœ… VALIDATION PASSED - Week 3 consolidation successfully validated")
            print("ğŸš€ Core ML infrastructure consolidation is functional")
        else:
            print("âŒ VALIDATION NEEDS ATTENTION - Some consolidation issues found")
            print("ğŸ”§ Review test results for specific component issues")
        
        print(f"\nğŸ“Š Validation Summary:")
        print(f"  â€¢ Total Tests: {summary.get('total_tests', 0)}")
        print(f"  â€¢ Successful: {summary.get('successful_tests', 0)}")
        print(f"  â€¢ Success Rate: {summary.get('success_rate', 0):.1%}")
        print(f"  â€¢ Consolidation Validated: {'âœ… Yes' if summary.get('consolidation_validated', False) else 'âŒ No'}")
        print(f"  â€¢ Execution Time: {summary.get('execution_time_seconds', 0):.2f}s")
        
        print(f"\nğŸ“‹ Test Results:")
        for test in results.get("tests", []):
            status = "âœ…" if test.get("success", False) else "âŒ"
            name = test.get("test_name", "Unknown")
            time_taken = test.get("execution_time", 0)
            print(f"  {status} {name}: {time_taken:.2f}s")
            
            if not test.get("success", False) and "error" in test:
                print(f"      âš ï¸  Error: {test['error']}")
        
        # Show consolidation analysis
        consolidation = results.get("consolidation_analysis", {})
        if consolidation:
            print(f"\nğŸ—ï¸  Consolidation Analysis:")
            arch_improvements = consolidation.get("architectural_improvements", {})
            for component, details in arch_improvements.items():
                validated = details.get("validated", False)
                status = "âœ…" if validated else "âŒ"
                print(f"  {status} {component.replace('_', ' ').title()}: {details.get('description', 'N/A')}")
        
        print(f"\nğŸ“ Detailed results saved to: {results_file.absolute()}")
        
        if summary.get("overall_success", False):
            print("\nğŸ‰ Ready to proceed with Week 4 API layer standardization!")
        else:
            print("\nâš ï¸  Address issues before proceeding to Week 4")
        
        print("="*75)
        
        return results
        
    except Exception as e:
        logger.error(f"Validation execution failed: {e}")
        return {"success": False, "error": str(e)}

if __name__ == "__main__":
    asyncio.run(main())