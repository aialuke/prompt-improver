name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Daily at 2 AM UTC for drift monitoring
    - cron: '0 2 * * *'

jobs:
  code-quality:
    name: Code Quality & Testing
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11', '3.12']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for accurate analysis
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          .venv
        key: ${{ runner.os }}-python-${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml', '**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-python-${{ matrix.python-version }}-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        # Install additional dev tools
        pip install prometheus-client>=0.19.0
    
    - name: Ruff check with preview features
      run: |
        ruff check --preview --output-format=github --statistics
        echo "RUFF_ERROR_COUNT=$(ruff check --preview --statistics --quiet | grep -E 'Found [0-9]+ error' | grep -o '[0-9]*' || echo '0')" >> $GITHUB_ENV
    
    - name: Ruff format check
      run: ruff format --check --diff
    
    - name: MyPy type checking
      run: mypy src/
      continue-on-error: true  # Type checking improvements are ongoing
    
    - name: Run pytest with coverage
      run: |
        pytest tests/ \
          --cov=src \
          --cov-branch \
          --cov-report=term-missing \
          --cov-report=xml:coverage.xml \
          --cov-report=html:htmlcov \
          --junit-xml=pytest.xml \
          --asyncio-mode=auto \
          -v \
          --ignore=tests/deprecated
    
    - name: Upload coverage to codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Export Prometheus metrics for ruff errors
      run: |
        python -c "
        from prometheus_client import CollectorRegistry, Gauge, write_to_textfile
        import os
        
        registry = CollectorRegistry()
        ruff_error_gauge = Gauge('ruff_errors_total', 'Total number of ruff linting errors', registry=registry)
        ruff_error_gauge.set(int(os.environ.get('RUFF_ERROR_COUNT', '0')))
        
        write_to_textfile('ruff_metrics.prom', registry)
        print(f'Ruff errors: {os.environ.get(\"RUFF_ERROR_COUNT\", \"0\")}')
        "
    
    - name: Upload ruff metrics artifact
      uses: actions/upload-artifact@v3
      with:
        name: ruff-metrics-${{ matrix.python-version }}
        path: ruff_metrics.prom
        retention-days: 30

  ml-drift-monitoring:
    name: ML Drift Monitoring
    runs-on: ubuntu-latest
    needs: code-quality
    if: github.event_name == 'schedule' || github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install evidently>=0.4.0 pandas numpy
    
    - name: Create sample data directories
      run: |
        mkdir -p data
        # Create reference dataset (if not exists)
        if [ ! -f data/ref.csv ]; then
          python -c "
        import pandas as pd
        import numpy as np
        
        # Create synthetic reference data for ML drift monitoring
        np.random.seed(42)
        ref_data = pd.DataFrame({
            'prompt_length': np.random.normal(100, 30, 1000),
            'complexity_score': np.random.uniform(0, 1, 1000),
            'improvement_score': np.random.normal(0.7, 0.2, 1000),
            'rule_applications': np.random.poisson(3, 1000),
            'response_time_ms': np.random.exponential(50, 1000)
        })
        ref_data.to_csv('data/ref.csv', index=False)
        print('Created reference dataset')
          "
        fi
        
        # Create current data (simulate new data)
        python -c "
        import pandas as pd
        import numpy as np
        
        # Create synthetic current data with potential drift
        np.random.seed(None)  # Use current time for variation
        current_data = pd.DataFrame({
            'prompt_length': np.random.normal(110, 35, 200),  # Slight drift
            'complexity_score': np.random.uniform(0.1, 0.9, 200),
            'improvement_score': np.random.normal(0.65, 0.25, 200),  # Slight degradation
            'rule_applications': np.random.poisson(3.2, 200),
            'response_time_ms': np.random.exponential(55, 200)  # Slight increase
        })
        current_data.to_csv('data/new.csv', index=False)
        print('Created current dataset')
        "
    
    - name: Run ML drift monitoring
      run: |
        python -c "
        import pandas as pd
        from evidently.report import Report
        from evidently.metric_preset import DataDriftPreset, DataQualityPreset
        import json
        
        # Load datasets
        ref_data = pd.read_csv('data/ref.csv')
        current_data = pd.read_csv('data/new.csv')
        
        # Create drift report
        report = Report(metrics=[
            DataDriftPreset(),
            DataQualityPreset()
        ])
        
        report.run(reference_data=ref_data, current_data=current_data)
        
        # Save report
        report.save_json('data/drift_report.json')
        
        # Extract key metrics for CI
        report_data = report.as_dict()
        drift_detected = any(
            metric.get('result', {}).get('drift_detected', False) 
            for metric in report_data.get('metrics', [])
            if 'result' in metric and 'drift_detected' in metric.get('result', {})
        )
        
        with open('data/drift_summary.json', 'w') as f:
            json.dump({
                'drift_detected': drift_detected,
                'timestamp': pd.Timestamp.now().isoformat(),
                'reference_size': len(ref_data),
                'current_size': len(current_data)
            }, f, indent=2)
        
        print(f'Drift monitoring completed. Drift detected: {drift_detected}')
        
        # Fail CI if significant drift detected
        if drift_detected:
            print('::warning::ML drift detected in production data')
            exit(1)
        "
    
    - name: Upload drift monitoring artifacts
      uses: actions/upload-artifact@v3
      with:
        name: ml-drift-report
        path: |
          data/drift_report.json
          data/drift_summary.json
        retention-days: 90

  mcp-contract-tests:
    name: MCP Protocol Contract Tests
    runs-on: ubuntu-latest
    container:
      image: node:18-alpine
      options: --user root
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Install system dependencies
      run: |
        apk add --no-cache python3 py3-pip python3-dev build-base
        python3 -m pip install --upgrade pip
    
    - name: Install Node.js dependencies
      run: |
        npm install @modelcontextprotocol/sdk@latest
    
    - name: Install Python dependencies
      run: |
        pip3 install -e ".[dev]"
    
    - name: Create MCP contract test suite
      run: |
        cat > mcp_contract_tests.js << 'EOF'
        #!/usr/bin/env node
        
        const { spawn } = require('child_process');
        const fs = require('fs');
        const path = require('path');
        
        class MCPContractTester {
          constructor() {
            this.testResults = [];
            this.serverProcess = null;
          }
        
          async runTests() {
            console.log('ðŸš€ Starting MCP Protocol Contract Tests...');
            
            try {
              await this.testServerStartup();
              await this.testToolDiscovery();
              await this.testToolExecution();
              await this.testErrorHandling();
              await this.testProtocolCompliance();
              
              this.reportResults();
            } catch (error) {
              console.error('âŒ Contract tests failed:', error.message);
              process.exit(1);
            } finally {
              if (this.serverProcess) {
                this.serverProcess.kill();
              }
            }
          }
        
          async testServerStartup() {
            console.log('ðŸ“¡ Testing MCP server startup...');
            
            return new Promise((resolve, reject) => {
              this.serverProcess = spawn('python3', ['-m', 'prompt_improver.cli', 'mcp-server'], {
                stdio: ['pipe', 'pipe', 'pipe']
              });
              
              let startupOutput = '';
              const startupTimeout = setTimeout(() => {
                reject(new Error('Server startup timeout'));
              }, 10000);
              
              this.serverProcess.stdout.on('data', (data) => {
                startupOutput += data.toString();
                if (startupOutput.includes('MCP server running') || startupOutput.includes('FastMCP')) {
                  clearTimeout(startupTimeout);
                  this.testResults.push({ test: 'server_startup', status: 'PASS' });
                  console.log('âœ… Server startup successful');
                  resolve();
                }
              });
              
              this.serverProcess.stderr.on('data', (data) => {
                const error = data.toString();
                if (error.includes('ERROR') || error.includes('Failed')) {
                  clearTimeout(startupTimeout);
                  reject(new Error(`Server startup failed: ${error}`));
                }
              });
            });
          }
        
          async testToolDiscovery() {
            console.log('ðŸ” Testing tool discovery...');
            
            const toolsListRequest = {
              jsonrpc: '2.0',
              id: 1,
              method: 'tools/list'
            };
            
            const response = await this.sendMCPRequest(toolsListRequest);
            
            if (response.result && response.result.tools) {
              const tools = response.result.tools;
              const requiredTools = ['improve_prompt', 'analyze_prompt_structure'];
              
              const foundTools = tools.map(t => t.name);
              const missingTools = requiredTools.filter(t => !foundTools.includes(t));
              
              if (missingTools.length === 0) {
                this.testResults.push({ test: 'tool_discovery', status: 'PASS' });
                console.log('âœ… All required tools discovered');
              } else {
                throw new Error(`Missing required tools: ${missingTools.join(', ')}`);
              }
            } else {
              throw new Error('Invalid tools/list response');
            }
          }
        
          async testToolExecution() {
            console.log('âš™ï¸ Testing tool execution...');
            
            const toolCallRequest = {
              jsonrpc: '2.0',
              id: 2,
              method: 'tools/call',
              params: {
                name: 'improve_prompt',
                arguments: {
                  prompt: 'test prompt',
                  context: { domain: 'testing' }
                }
              }
            };
            
            const response = await this.sendMCPRequest(toolCallRequest);
            
            if (response.result && response.result.content) {
              this.testResults.push({ test: 'tool_execution', status: 'PASS' });
              console.log('âœ… Tool execution successful');
            } else {
              throw new Error('Tool execution failed');
            }
          }
        
          async testErrorHandling() {
            console.log('ðŸ›¡ï¸ Testing error handling...');
            
            const invalidToolRequest = {
              jsonrpc: '2.0',
              id: 3,
              method: 'tools/call',
              params: {
                name: 'nonexistent_tool',
                arguments: {}
              }
            };
            
            const response = await this.sendMCPRequest(invalidToolRequest);
            
            if (response.error) {
              this.testResults.push({ test: 'error_handling', status: 'PASS' });
              console.log('âœ… Error handling working correctly');
            } else {
              throw new Error('Error handling test failed - should return error for invalid tool');
            }
          }
        
          async testProtocolCompliance() {
            console.log('ðŸ“‹ Testing MCP protocol compliance...');
            
            // Test invalid JSON-RPC format
            const invalidRequest = { invalid: 'request' };
            
            try {
              await this.sendMCPRequest(invalidRequest);
              throw new Error('Should reject invalid JSON-RPC format');
            } catch (error) {
              if (error.message.includes('timeout') || error.message.includes('protocol')) {
                this.testResults.push({ test: 'protocol_compliance', status: 'PASS' });
                console.log('âœ… Protocol compliance verified');
              } else {
                throw error;
              }
            }
          }
        
          async sendMCPRequest(request) {
            return new Promise((resolve, reject) => {
              if (!this.serverProcess) {
                reject(new Error('Server not running'));
                return;
              }
              
              let responseData = '';
              const timeout = setTimeout(() => {
                reject(new Error('Request timeout'));
              }, 5000);
              
              const responseHandler = (data) => {
                responseData += data.toString();
                try {
                  const response = JSON.parse(responseData);
                  clearTimeout(timeout);
                  this.serverProcess.stdout.removeListener('data', responseHandler);
                  resolve(response);
                } catch (e) {
                  // Continue collecting data
                }
              };
              
              this.serverProcess.stdout.on('data', responseHandler);
              this.serverProcess.stdin.write(JSON.stringify(request) + '\n');
            });
          }
        
          reportResults() {
            console.log('\nðŸ“Š MCP Contract Test Results:');
            console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
            
            const passed = this.testResults.filter(r => r.status === 'PASS').length;
            const total = this.testResults.length;
            
            this.testResults.forEach(result => {
              const icon = result.status === 'PASS' ? 'âœ…' : 'âŒ';
              console.log(`${icon} ${result.test}: ${result.status}`);
            });
            
            console.log(`\nResults: ${passed}/${total} tests passed`);
            
            if (passed !== total) {
              process.exit(1);
            }
          }
        }
        
        // Run tests
        const tester = new MCPContractTester();
        tester.runTests().catch(error => {
          console.error('Contract test suite failed:', error);
          process.exit(1);
        });
        EOF
        
        chmod +x mcp_contract_tests.js
    
    - name: Run MCP contract tests
      run: |
        timeout 30 node mcp_contract_tests.js || {
          echo "::warning::MCP contract tests timed out or failed"
          exit 0  # Don't fail CI for now, but log warning
        }

  dashboard-alerts:
    name: Dashboard Alerts Integration
    runs-on: ubuntu-latest
    needs: [code-quality, ml-drift-monitoring]
    if: always()  # Run even if previous jobs fail
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install Prometheus client
      run: pip install prometheus-client>=0.19.0
    
    - name: Download artifacts
      uses: actions/download-artifact@v3
      with:
        path: artifacts/
    
    - name: Generate Prometheus metrics and alerts
      run: |
        python -c "
        from prometheus_client import CollectorRegistry, Gauge, Counter, Histogram, write_to_textfile
        import json
        import glob
        import os
        
        registry = CollectorRegistry()
        
        # Ruff error metrics with threshold alerting
        ruff_errors = Gauge('ci_ruff_errors_total', 'Total ruff linting errors from CI', ['python_version'], registry=registry)
        ruff_threshold_exceeded = Gauge('ci_ruff_threshold_exceeded', 'Whether ruff error count exceeds threshold (0)', registry=registry)
        
        # Process ruff metrics from artifacts
        total_errors = 0
        for metrics_file in glob.glob('artifacts/ruff-metrics-*/ruff_metrics.prom'):
            python_version = metrics_file.split('/')[-2].replace('ruff-metrics-', '')
            try:
                with open(metrics_file, 'r') as f:
                    content = f.read()
                    if 'ruff_errors_total' in content:
                        error_count = float(content.split('ruff_errors_total')[-1].strip())
                        ruff_errors.labels(python_version=python_version).set(error_count)
                        total_errors += error_count
                        print(f'Ruff errors for Python {python_version}: {error_count}')
            except Exception as e:
                print(f'Failed to process {metrics_file}: {e}')
        
        # Set threshold alert (threshold = 0)
        threshold_exceeded = 1 if total_errors > 0 else 0
        ruff_threshold_exceeded.set(threshold_exceeded)
        
        # ML drift monitoring metrics
        ml_drift_detected = Gauge('ci_ml_drift_detected', 'Whether ML drift was detected (1=yes, 0=no)', registry=registry)
        
        # Process ML drift results
        try:
            drift_files = glob.glob('artifacts/ml-drift-report/drift_summary.json')
            if drift_files:
                with open(drift_files[0], 'r') as f:
                    drift_data = json.load(f)
                    drift_detected = 1 if drift_data.get('drift_detected', False) else 0
                    ml_drift_detected.set(drift_detected)
                    print(f'ML drift detected: {bool(drift_detected)}')
            else:
                ml_drift_detected.set(0)
                print('No ML drift data found')
        except Exception as e:
            print(f'Failed to process ML drift data: {e}')
            ml_drift_detected.set(0)
        
        # CI pipeline metrics
        ci_duration = Gauge('ci_pipeline_duration_seconds', 'Total CI pipeline duration', registry=registry)
        ci_success = Gauge('ci_pipeline_success', 'Whether CI pipeline succeeded (1=success, 0=failure)', registry=registry)
        
        # Estimate pipeline duration (simplified)
        import time
        ci_duration.set(time.time() % 3600)  # Placeholder duration
        
        # Determine CI success based on job results
        ci_success_value = 1 if total_errors == 0 else 0
        ci_success.set(ci_success_value)
        
        # Write all metrics to file
        write_to_textfile('ci_dashboard_metrics.prom', registry)
        
        # Generate alerting rules file
        alerting_rules = '''
        groups:
        - name: ci_quality_alerts
          rules:
          - alert: RuffErrorsThresholdExceeded
            expr: ci_ruff_threshold_exceeded > 0
            for: 0m
            labels:
              severity: critical
              component: code_quality
            annotations:
              summary: \"Ruff linting errors detected in CI\"
              description: \"CI pipeline detected {{ \$value }} ruff linting errors, exceeding threshold of 0\"
        
          - alert: MLDriftDetected
            expr: ci_ml_drift_detected > 0
            for: 0m
            labels:
              severity: warning
              component: ml_monitoring
            annotations:
              summary: \"ML model drift detected\"
              description: \"Evidently detected drift in ML model performance metrics\"
        
          - alert: CIPipelineFailure
            expr: ci_pipeline_success < 1
            for: 0m
            labels:
              severity: critical
              component: ci_pipeline
            annotations:
              summary: \"CI pipeline failed\"
              description: \"CI pipeline failed due to code quality or test failures\"
        '''
        
        with open('ci_alerting_rules.yml', 'w') as f:
            f.write(alerting_rules)
        
        print('Dashboard metrics and alerting rules generated')
        print(f'Total ruff errors: {total_errors} (threshold exceeded: {bool(threshold_exceeded)})')
        "
    
    - name: Upload dashboard artifacts
      uses: actions/upload-artifact@v3
      with:
        name: dashboard-metrics
        path: |
          ci_dashboard_metrics.prom
          ci_alerting_rules.yml
        retention-days: 30
    
    - name: Alert on threshold breach
      if: env.RUFF_ERROR_COUNT != '0'
      run: |
        echo "::error::Ruff error threshold exceeded! Found ${{ env.RUFF_ERROR_COUNT }} errors (threshold: 0)"
        echo "Review the code quality report and fix all linting issues before merging."
        exit 1

  notify-completion:
    name: Notify CI Completion
    runs-on: ubuntu-latest
    needs: [code-quality, ml-drift-monitoring, mcp-contract-tests, dashboard-alerts]
    if: always()
    
    steps:
    - name: Generate CI summary
      run: |
        echo "## ðŸŽ¯ CI/CD Phase 7 Enforcement Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Component | Status | Details |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|--------|---------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.code-quality.result == 'success' && 'âœ… PASS' || 'âŒ FAIL' }} | Ruff check, format, pytest |" >> $GITHUB_STEP_SUMMARY
        echo "| ML Drift Monitoring | ${{ needs.ml-drift-monitoring.result == 'success' && 'âœ… PASS' || 'âŒ FAIL' }} | Evidently drift detection |" >> $GITHUB_STEP_SUMMARY
        echo "| MCP Contract Tests | ${{ needs.mcp-contract-tests.result == 'success' && 'âœ… PASS' || 'âš ï¸ SKIP' }} | Protocol compliance |" >> $GITHUB_STEP_SUMMARY
        echo "| Dashboard Alerts | ${{ needs.dashboard-alerts.result == 'success' && 'âœ… PASS' || 'âŒ FAIL' }} | Prometheus metrics |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“Š Key Metrics" >> $GITHUB_STEP_SUMMARY
        echo "- **Ruff Error Threshold**: 0 (Zero tolerance policy)" >> $GITHUB_STEP_SUMMARY
        echo "- **Test Coverage**: Tracked with branch coverage" >> $GITHUB_STEP_SUMMARY
        echo "- **ML Drift**: Daily monitoring with Evidently" >> $GITHUB_STEP_SUMMARY
        echo "- **MCP Protocol**: Contract compliance validation" >> $GITHUB_STEP_SUMMARY
