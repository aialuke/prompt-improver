"""Baseline automation system for scheduled collection and analysis."""

import asyncio
import json
import logging
import time
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable, Union, Tuple
import uuid

# Scheduling
try:\n    import aiocron\n    AIOCRON_AVAILABLE = True\nexcept ImportError:\n    AIOCRON_AVAILABLE = False\n\n# Task queue\ntry:\n    import celery\n    CELERY_AVAILABLE = True\nexcept ImportError:\n    CELERY_AVAILABLE = False\n\nfrom .baseline_collector import BaselineCollector\nfrom .statistical_analyzer import StatisticalAnalyzer\nfrom .regression_detector import RegressionDetector\nfrom .profiler import ContinuousProfiler\nfrom .models import BaselineMetrics, RegressionAlert, PerformanceTrend\n\nlogger = logging.getLogger(__name__)\n\nclass AutomationConfig:\n    \"\"\"Configuration for baseline automation.\"\"\"\n    \n    def __init__(\n        self,\n        collection_schedule: str = \"*/5 * * * *\",  # Every 5 minutes\n        analysis_schedule: str = \"0 */1 * * *\",   # Every hour\n        reporting_schedule: str = \"0 9 * * *\",    # Daily at 9 AM\n        regression_check_schedule: str = \"*/10 * * * *\",  # Every 10 minutes\n        \n        # Retention settings\n        baseline_retention_days: int = 30,\n        analysis_retention_days: int = 90,\n        alert_retention_days: int = 365,\n        \n        # Analysis settings\n        trend_analysis_hours: int = 24,\n        regression_reference_hours: int = 168,  # 7 days\n        min_baselines_for_analysis: int = 10,\n        \n        # Automation flags\n        enable_auto_collection: bool = True,\n        enable_auto_analysis: bool = True,\n        enable_auto_regression_detection: bool = True,\n        enable_auto_profiling: bool = False,\n        enable_auto_reporting: bool = True,\n        \n        # Storage settings\n        storage_path: Optional[Path] = None,\n        backup_enabled: bool = True,\n        backup_schedule: str = \"0 2 * * *\"  # Daily at 2 AM\n    ):\n        \"\"\"Initialize automation configuration.\n        \n        Args:\n            collection_schedule: Cron schedule for baseline collection\n            analysis_schedule: Cron schedule for trend analysis\n            reporting_schedule: Cron schedule for automated reporting\n            regression_check_schedule: Cron schedule for regression detection\n            baseline_retention_days: Days to retain baseline data\n            analysis_retention_days: Days to retain analysis results\n            alert_retention_days: Days to retain alert history\n            trend_analysis_hours: Hours of data for trend analysis\n            regression_reference_hours: Hours of reference data for regression detection\n            min_baselines_for_analysis: Minimum baselines required for analysis\n            enable_auto_collection: Enable automatic baseline collection\n            enable_auto_analysis: Enable automatic trend analysis\n            enable_auto_regression_detection: Enable automatic regression detection\n            enable_auto_profiling: Enable automatic profiling\n            enable_auto_reporting: Enable automated reporting\n            storage_path: Base path for storing automation data\n            backup_enabled: Enable automated backups\n            backup_schedule: Cron schedule for backups\n        \"\"\"\n        self.collection_schedule = collection_schedule\n        self.analysis_schedule = analysis_schedule\n        self.reporting_schedule = reporting_schedule\n        self.regression_check_schedule = regression_check_schedule\n        \n        self.baseline_retention_days = baseline_retention_days\n        self.analysis_retention_days = analysis_retention_days\n        self.alert_retention_days = alert_retention_days\n        \n        self.trend_analysis_hours = trend_analysis_hours\n        self.regression_reference_hours = regression_reference_hours\n        self.min_baselines_for_analysis = min_baselines_for_analysis\n        \n        self.enable_auto_collection = enable_auto_collection\n        self.enable_auto_analysis = enable_auto_analysis\n        self.enable_auto_regression_detection = enable_auto_regression_detection\n        self.enable_auto_profiling = enable_auto_profiling\n        self.enable_auto_reporting = enable_auto_reporting\n        \n        self.storage_path = storage_path or Path(\"./automation\")\n        self.storage_path.mkdir(parents=True, exist_ok=True)\n        \n        self.backup_enabled = backup_enabled\n        self.backup_schedule = backup_schedule\n\nclass AutomationTaskResult:\n    \"\"\"Result of an automation task.\"\"\"\n    \n    def __init__(\n        self,\n        task_id: str,\n        task_type: str,\n        status: str,\n        start_time: datetime,\n        end_time: datetime,\n        result_data: Optional[Dict[str, Any]] = None,\n        error_message: Optional[str] = None\n    ):\n        self.task_id = task_id\n        self.task_type = task_type\n        self.status = status  # 'success', 'failed', 'partial'\n        self.start_time = start_time\n        self.end_time = end_time\n        self.result_data = result_data or {}\n        self.error_message = error_message\n        \n        self.duration_seconds = (end_time - start_time).total_seconds()\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            'task_id': self.task_id,\n            'task_type': self.task_type,\n            'status': self.status,\n            'start_time': self.start_time.isoformat(),\n            'end_time': self.end_time.isoformat(),\n            'duration_seconds': self.duration_seconds,\n            'result_data': self.result_data,\n            'error_message': self.error_message\n        }\n\nclass BaselineAutomation:\n    \"\"\"Automated baseline collection, analysis, and monitoring system.\n    \n    Orchestrates all performance baseline activities including scheduled collection,\n    trend analysis, regression detection, and automated reporting.\n    \"\"\"\n    \n    def __init__(\n        self,\n        config: Optional[AutomationConfig] = None,\n        collector: Optional[BaselineCollector] = None,\n        analyzer: Optional[StatisticalAnalyzer] = None,\n        detector: Optional[RegressionDetector] = None,\n        profiler: Optional[ContinuousProfiler] = None\n    ):\n        \"\"\"Initialize baseline automation.\n        \n        Args:\n            config: Automation configuration\n            collector: Baseline collector instance\n            analyzer: Statistical analyzer instance\n            detector: Regression detector instance\n            profiler: Continuous profiler instance\n        \"\"\"\n        self.config = config or AutomationConfig()\n        \n        # Components\n        self.collector = collector or BaselineCollector()\n        self.analyzer = analyzer or StatisticalAnalyzer()\n        self.detector = detector or RegressionDetector()\n        self.profiler = profiler or ContinuousProfiler()\n        \n        # State management\n        self._running = False\n        self._scheduled_tasks: Dict[str, Any] = {}\n        self._task_history: List[AutomationTaskResult] = []\n        self._lock = asyncio.Lock()\n        \n        # Storage paths\n        self.baselines_path = self.config.storage_path / \"baselines\"\n        self.analysis_path = self.config.storage_path / \"analysis\"\n        self.alerts_path = self.config.storage_path / \"alerts\"\n        self.reports_path = self.config.storage_path / \"reports\"\n        \n        # Create directories\n        for path in [self.baselines_path, self.analysis_path, self.alerts_path, self.reports_path]:\n            path.mkdir(parents=True, exist_ok=True)\n        \n        logger.info(f\"BaselineAutomation initialized with storage at {self.config.storage_path}\")\n\n    async def start_automation(self) -> None:\n        \"\"\"Start all automated processes.\"\"\"\n        if self._running:\n            logger.warning(\"Automation already running\")\n            return\n        \n        self._running = True\n        \n        # Start baseline collection\n        if self.config.enable_auto_collection:\n            await self.collector.start_collection()\n        \n        # Start continuous profiling if enabled\n        if self.config.enable_auto_profiling:\n            await self.profiler.start_profiling()\n        \n        # Schedule automated tasks\n        await self._schedule_automation_tasks()\n        \n        logger.info(\"Started baseline automation\")\n\n    async def stop_automation(self) -> None:\n        \"\"\"Stop all automated processes.\"\"\"\n        if not self._running:\n            return\n        \n        self._running = False\n        \n        # Stop collection and profiling\n        await self.collector.stop_collection()\n        await self.profiler.stop_profiling()\n        \n        # Cancel scheduled tasks\n        await self._cancel_scheduled_tasks()\n        \n        logger.info(\"Stopped baseline automation\")\n\n    async def _schedule_automation_tasks(self) -> None:\n        \"\"\"Schedule all automation tasks.\"\"\"\n        if not AIOCRON_AVAILABLE:\n            logger.warning(\"aiocron not available, using fallback scheduling\")\n            # Start fallback task loop\n            asyncio.create_task(self._fallback_task_loop())\n            return\n        \n        try:\n            # Schedule trend analysis\n            if self.config.enable_auto_analysis:\n                analysis_task = aiocron.crontab(\n                    self.config.analysis_schedule,\n                    func=self._run_trend_analysis,\n                    start=True\n                )\n                self._scheduled_tasks['analysis'] = analysis_task\n                logger.info(f\"Scheduled trend analysis: {self.config.analysis_schedule}\")\n            \n            # Schedule regression detection\n            if self.config.enable_auto_regression_detection:\n                regression_task = aiocron.crontab(\n                    self.config.regression_check_schedule,\n                    func=self._run_regression_detection,\n                    start=True\n                )\n                self._scheduled_tasks['regression'] = regression_task\n                logger.info(f\"Scheduled regression detection: {self.config.regression_check_schedule}\")\n            \n            # Schedule reporting\n            if self.config.enable_auto_reporting:\n                reporting_task = aiocron.crontab(\n                    self.config.reporting_schedule,\n                    func=self._run_automated_reporting,\n                    start=True\n                )\n                self._scheduled_tasks['reporting'] = reporting_task\n                logger.info(f\"Scheduled automated reporting: {self.config.reporting_schedule}\")\n            \n            # Schedule cleanup\n            cleanup_task = aiocron.crontab(\n                \"0 1 * * *\",  # Daily at 1 AM\n                func=self._run_cleanup,\n                start=True\n            )\n            self._scheduled_tasks['cleanup'] = cleanup_task\n            \n            # Schedule backup if enabled\n            if self.config.backup_enabled:\n                backup_task = aiocron.crontab(\n                    self.config.backup_schedule,\n                    func=self._run_backup,\n                    start=True\n                )\n                self._scheduled_tasks['backup'] = backup_task\n                logger.info(f\"Scheduled backup: {self.config.backup_schedule}\")\n        \n        except Exception as e:\n            logger.error(f\"Failed to schedule automation tasks: {e}\")\n\n    async def _cancel_scheduled_tasks(self) -> None:\n        \"\"\"Cancel all scheduled tasks.\"\"\"\n        for task_name, task in self._scheduled_tasks.items():\n            try:\n                if hasattr(task, 'stop'):\n                    task.stop()\n                elif hasattr(task, 'cancel'):\n                    task.cancel()\n                logger.debug(f\"Cancelled scheduled task: {task_name}\")\n            except Exception as e:\n                logger.error(f\"Error cancelling task {task_name}: {e}\")\n        \n        self._scheduled_tasks.clear()\n\n    async def _fallback_task_loop(self) -> None:\n        \"\"\"Fallback task loop when aiocron is not available.\"\"\"\n        logger.info(\"Starting fallback task scheduling\")\n        \n        last_analysis = datetime.min.replace(tzinfo=timezone.utc)\n        last_regression_check = datetime.min.replace(tzinfo=timezone.utc)\n        last_reporting = datetime.min.replace(tzinfo=timezone.utc)\n        last_cleanup = datetime.min.replace(tzinfo=timezone.utc)\n        \n        while self._running:\n            try:\n                current_time = datetime.now(timezone.utc)\n                \n                # Trend analysis (every hour)\n                if self.config.enable_auto_analysis and (current_time - last_analysis).total_seconds() >= 3600:\n                    await self._run_trend_analysis()\n                    last_analysis = current_time\n                \n                # Regression detection (every 10 minutes)\n                if self.config.enable_auto_regression_detection and (current_time - last_regression_check).total_seconds() >= 600:\n                    await self._run_regression_detection()\n                    last_regression_check = current_time\n                \n                # Daily reporting (once per day)\n                if self.config.enable_auto_reporting and (current_time - last_reporting).total_seconds() >= 86400:\n                    if current_time.hour == 9:  # 9 AM\n                        await self._run_automated_reporting()\n                        last_reporting = current_time\n                \n                # Daily cleanup (once per day)\n                if (current_time - last_cleanup).total_seconds() >= 86400:\n                    if current_time.hour == 1:  # 1 AM\n                        await self._run_cleanup()\n                        last_cleanup = current_time\n                \n                # Wait before next check\n                await asyncio.sleep(60)  # Check every minute\n                \n            except Exception as e:\n                logger.error(f\"Error in fallback task loop: {e}\")\n                await asyncio.sleep(60)\n\n    async def _run_trend_analysis(self) -> None:\n        \"\"\"Run automated trend analysis.\"\"\"\n        task_id = str(uuid.uuid4())\n        start_time = datetime.now(timezone.utc)\n        \n        logger.info(f\"Starting automated trend analysis (task: {task_id})\")\n        \n        try:\n            # Load recent baselines\n            baselines = await self.collector.load_recent_baselines(\n                hours=self.config.trend_analysis_hours\n            )\n            \n            if len(baselines) < self.config.min_baselines_for_analysis:\n                logger.warning(f\"Insufficient baselines for analysis ({len(baselines)} < {self.config.min_baselines_for_analysis})\")\n                return\n            \n            # Analyze trends for key metrics\n            metrics_to_analyze = [\n                'response_time', 'error_rate', 'throughput', \n                'cpu_utilization', 'memory_utilization'\n            ]\n            \n            analysis_results = {}\n            \n            for metric_name in metrics_to_analyze:\n                try:\n                    trend = await self.analyzer.analyze_trend(\n                        metric_name, baselines, self.config.trend_analysis_hours\n                    )\n                    analysis_results[metric_name] = {\n                        'direction': trend.direction.value,\n                        'magnitude': trend.magnitude,\n                        'confidence_score': trend.confidence_score,\n                        'sample_count': trend.sample_count,\n                        'is_significant': trend.is_significant(),\n                        'predicted_24h': trend.predicted_value_24h,\n                        'predicted_7d': trend.predicted_value_7d\n                    }\n                except Exception as e:\n                    logger.error(f\"Failed to analyze trend for {metric_name}: {e}\")\n            \n            # Save analysis results\n            await self._save_analysis_results(task_id, analysis_results)\n            \n            # Record task result\n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='trend_analysis',\n                status='success',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                result_data={\n                    'metrics_analyzed': len(analysis_results),\n                    'baselines_used': len(baselines),\n                    'timeframe_hours': self.config.trend_analysis_hours\n                }\n            )\n            \n            await self._record_task_result(result)\n            \n            logger.info(f\"Completed trend analysis (task: {task_id}): {len(analysis_results)} metrics\")\n        \n        except Exception as e:\n            logger.error(f\"Trend analysis failed (task: {task_id}): {e}\")\n            \n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='trend_analysis',\n                status='failed',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                error_message=str(e)\n            )\n            \n            await self._record_task_result(result)\n\n    async def _run_regression_detection(self) -> None:\n        \"\"\"Run automated regression detection.\"\"\"\n        task_id = str(uuid.uuid4())\n        start_time = datetime.now(timezone.utc)\n        \n        logger.debug(f\"Starting regression detection (task: {task_id})\")\n        \n        try:\n            # Get current baseline\n            recent_baselines = await self.collector.load_recent_baselines(hours=1)\n            if not recent_baselines:\n                logger.debug(\"No recent baselines for regression detection\")\n                return\n            \n            current_baseline = recent_baselines[-1]  # Most recent\n            \n            # Get reference baselines\n            reference_baselines = await self.collector.load_recent_baselines(\n                hours=self.config.regression_reference_hours\n            )\n            \n            if len(reference_baselines) < self.config.min_baselines_for_analysis:\n                logger.debug(f\"Insufficient reference baselines ({len(reference_baselines)})\")\n                return\n            \n            # Check for regressions\n            alerts = await self.detector.check_for_regressions(\n                current_baseline, reference_baselines[:-1]  # Exclude current\n            )\n            \n            # Save alerts\n            if alerts:\n                await self._save_regression_alerts(task_id, alerts)\n            \n            # Record task result\n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='regression_detection',\n                status='success',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                result_data={\n                    'alerts_generated': len(alerts),\n                    'reference_baselines': len(reference_baselines),\n                    'current_baseline_id': current_baseline.baseline_id\n                }\n            )\n            \n            await self._record_task_result(result)\n            \n            if alerts:\n                logger.info(f\"Regression detection (task: {task_id}): {len(alerts)} alerts generated\")\n            else:\n                logger.debug(f\"Regression detection (task: {task_id}): No regressions detected\")\n        \n        except Exception as e:\n            logger.error(f\"Regression detection failed (task: {task_id}): {e}\")\n            \n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='regression_detection',\n                status='failed',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                error_message=str(e)\n            )\n            \n            await self._record_task_result(result)\n\n    async def _run_automated_reporting(self) -> None:\n        \"\"\"Run automated daily reporting.\"\"\"\n        task_id = str(uuid.uuid4())\n        start_time = datetime.now(timezone.utc)\n        \n        logger.info(f\"Starting automated reporting (task: {task_id})\")\n        \n        try:\n            # Generate daily performance report\n            report_data = await self._generate_daily_report()\n            \n            # Save report\n            await self._save_report(task_id, report_data)\n            \n            # Record task result\n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='automated_reporting',\n                status='success',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                result_data={\n                    'report_sections': len(report_data),\n                    'report_date': start_time.strftime('%Y-%m-%d')\n                }\n            )\n            \n            await self._record_task_result(result)\n            \n            logger.info(f\"Completed automated reporting (task: {task_id})\")\n        \n        except Exception as e:\n            logger.error(f\"Automated reporting failed (task: {task_id}): {e}\")\n            \n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='automated_reporting',\n                status='failed',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                error_message=str(e)\n            )\n            \n            await self._record_task_result(result)\n\n    async def _run_cleanup(self) -> None:\n        \"\"\"Run automated cleanup of old data.\"\"\"\n        task_id = str(uuid.uuid4())\n        start_time = datetime.now(timezone.utc)\n        \n        logger.info(f\"Starting automated cleanup (task: {task_id})\")\n        \n        try:\n            cleanup_stats = {\n                'baselines_removed': 0,\n                'analysis_removed': 0,\n                'alerts_removed': 0\n            }\n            \n            # Clean up old baselines\n            baseline_cutoff = datetime.now(timezone.utc) - timedelta(days=self.config.baseline_retention_days)\n            cleanup_stats['baselines_removed'] = await self._cleanup_old_files(\n                self.baselines_path, baseline_cutoff\n            )\n            \n            # Clean up old analysis results\n            analysis_cutoff = datetime.now(timezone.utc) - timedelta(days=self.config.analysis_retention_days)\n            cleanup_stats['analysis_removed'] = await self._cleanup_old_files(\n                self.analysis_path, analysis_cutoff\n            )\n            \n            # Clean up old alerts\n            alert_cutoff = datetime.now(timezone.utc) - timedelta(days=self.config.alert_retention_days)\n            cleanup_stats['alerts_removed'] = await self._cleanup_old_files(\n                self.alerts_path, alert_cutoff\n            )\n            \n            # Clean up task history\n            history_cutoff = datetime.now(timezone.utc) - timedelta(days=30)\n            initial_count = len(self._task_history)\n            self._task_history = [\n                task for task in self._task_history\n                if task.end_time > history_cutoff\n            ]\n            cleanup_stats['tasks_removed'] = initial_count - len(self._task_history)\n            \n            # Record task result\n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='cleanup',\n                status='success',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                result_data=cleanup_stats\n            )\n            \n            await self._record_task_result(result)\n            \n            logger.info(f\"Completed cleanup (task: {task_id}): {cleanup_stats}\")\n        \n        except Exception as e:\n            logger.error(f\"Cleanup failed (task: {task_id}): {e}\")\n            \n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='cleanup',\n                status='failed',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                error_message=str(e)\n            )\n            \n            await self._record_task_result(result)\n\n    async def _run_backup(self) -> None:\n        \"\"\"Run automated backup.\"\"\"\n        task_id = str(uuid.uuid4())\n        start_time = datetime.now(timezone.utc)\n        \n        logger.info(f\"Starting automated backup (task: {task_id})\")\n        \n        try:\n            # Create backup directory\n            backup_dir = self.config.storage_path / \"backups\" / start_time.strftime(\"%Y%m%d_%H%M%S\")\n            backup_dir.mkdir(parents=True, exist_ok=True)\n            \n            # Copy important files\n            import shutil\n            \n            backup_stats = {\n                'files_backed_up': 0,\n                'backup_size_mb': 0\n            }\n            \n            # Backup recent baselines (last 7 days)\n            recent_cutoff = datetime.now(timezone.utc) - timedelta(days=7)\n            for baseline_file in self.baselines_path.glob(\"baseline_*.json\"):\n                if baseline_file.stat().st_mtime > recent_cutoff.timestamp():\n                    shutil.copy2(baseline_file, backup_dir)\n                    backup_stats['files_backed_up'] += 1\n            \n            # Backup recent analysis results\n            for analysis_file in self.analysis_path.glob(\"analysis_*.json\"):\n                if analysis_file.stat().st_mtime > recent_cutoff.timestamp():\n                    shutil.copy2(analysis_file, backup_dir)\n                    backup_stats['files_backed_up'] += 1\n            \n            # Calculate backup size\n            backup_size = sum(f.stat().st_size for f in backup_dir.rglob('*') if f.is_file())\n            backup_stats['backup_size_mb'] = backup_size / (1024 * 1024)\n            \n            # Record task result\n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='backup',\n                status='success',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                result_data=backup_stats\n            )\n            \n            await self._record_task_result(result)\n            \n            logger.info(f\"Completed backup (task: {task_id}): {backup_stats}\")\n        \n        except Exception as e:\n            logger.error(f\"Backup failed (task: {task_id}): {e}\")\n            \n            result = AutomationTaskResult(\n                task_id=task_id,\n                task_type='backup',\n                status='failed',\n                start_time=start_time,\n                end_time=datetime.now(timezone.utc),\n                error_message=str(e)\n            )\n            \n            await self._record_task_result(result)\n\n    async def _save_analysis_results(self, task_id: str, results: Dict[str, Any]) -> None:\n        \"\"\"Save trend analysis results.\"\"\"\n        timestamp_str = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"analysis_{timestamp_str}_{task_id[:8]}.json\"\n        filepath = self.analysis_path / filename\n        \n        analysis_data = {\n            'task_id': task_id,\n            'timestamp': datetime.now(timezone.utc).isoformat(),\n            'analysis_type': 'trend_analysis',\n            'results': results\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(analysis_data, f, indent=2, default=str)\n\n    async def _save_regression_alerts(self, task_id: str, alerts: List[RegressionAlert]) -> None:\n        \"\"\"Save regression detection alerts.\"\"\"\n        timestamp_str = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"alerts_{timestamp_str}_{task_id[:8]}.json\"\n        filepath = self.alerts_path / filename\n        \n        alerts_data = {\n            'task_id': task_id,\n            'timestamp': datetime.now(timezone.utc).isoformat(),\n            'alert_count': len(alerts),\n            'alerts': [alert.to_dict() for alert in alerts]\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(alerts_data, f, indent=2, default=str)\n\n    async def _save_report(self, task_id: str, report_data: Dict[str, Any]) -> None:\n        \"\"\"Save automated report.\"\"\"\n        timestamp_str = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"report_{timestamp_str}_{task_id[:8]}.json\"\n        filepath = self.reports_path / filename\n        \n        with open(filepath, 'w') as f:\n            json.dump(report_data, f, indent=2, default=str)\n\n    async def _generate_daily_report(self) -> Dict[str, Any]:\n        \"\"\"Generate daily performance report.\"\"\"\n        report_date = datetime.now(timezone.utc).date()\n        \n        # Load recent baselines\n        baselines_24h = await self.collector.load_recent_baselines(hours=24)\n        baselines_7d = await self.collector.load_recent_baselines(hours=168)\n        \n        report = {\n            'report_date': report_date.isoformat(),\n            'generated_at': datetime.now(timezone.utc).isoformat(),\n            'summary': {\n                'baselines_24h': len(baselines_24h),\n                'baselines_7d': len(baselines_7d)\n            }\n        }\n        \n        if baselines_24h:\n            # Calculate performance metrics\n            latest_baseline = baselines_24h[-1]\n            \n            # Performance scores\n            if len(baselines_24h) > 1:\n                performance_score = await self.analyzer.calculate_performance_score(latest_baseline)\n                report['performance_score'] = performance_score\n            \n            # Recent alerts\n            report['recent_alerts'] = self.detector.get_active_alerts()\n            \n            # System statistics\n            report['statistics'] = {\n                'collector': self.collector.get_collection_status(),\n                'detector': self.detector.get_alert_statistics(),\n                'profiler': self.profiler.get_profiler_status()\n            }\n        \n        return report\n\n    async def _cleanup_old_files(self, directory: Path, cutoff_time: datetime) -> int:\n        \"\"\"Clean up files older than cutoff time.\"\"\"\n        removed_count = 0\n        \n        for file_path in directory.glob(\"*.json\"):\n            try:\n                file_time = datetime.fromtimestamp(file_path.stat().st_mtime, tz=timezone.utc)\n                if file_time < cutoff_time:\n                    file_path.unlink()\n                    removed_count += 1\n            except Exception as e:\n                logger.error(f\"Failed to remove {file_path}: {e}\")\n        \n        return removed_count\n\n    async def _record_task_result(self, result: AutomationTaskResult) -> None:\n        \"\"\"Record automation task result.\"\"\"\n        async with self._lock:\n            self._task_history.append(result)\n            \n            # Keep only recent history\n            if len(self._task_history) > 1000:\n                self._task_history = self._task_history[-1000:]\n\n    async def trigger_manual_analysis(self) -> AutomationTaskResult:\n        \"\"\"Manually trigger trend analysis.\"\"\"\n        logger.info(\"Manually triggering trend analysis\")\n        await self._run_trend_analysis()\n        \n        # Return the most recent analysis task result\n        analysis_tasks = [t for t in self._task_history if t.task_type == 'trend_analysis']\n        return analysis_tasks[-1] if analysis_tasks else None\n\n    async def trigger_manual_regression_check(self) -> AutomationTaskResult:\n        \"\"\"Manually trigger regression detection.\"\"\"\n        logger.info(\"Manually triggering regression detection\")\n        await self._run_regression_detection()\n        \n        # Return the most recent regression task result\n        regression_tasks = [t for t in self._task_history if t.task_type == 'regression_detection']\n        return regression_tasks[-1] if regression_tasks else None\n\n    def get_automation_status(self) -> Dict[str, Any]:\n        \"\"\"Get current automation status.\"\"\"\n        # Recent task statistics\n        recent_tasks = [\n            task for task in self._task_history\n            if (datetime.now(timezone.utc) - task.end_time).total_seconds() < 86400  # Last 24h\n        ]\n        \n        task_stats = {}\n        for task in recent_tasks:\n            task_type = task.task_type\n            if task_type not in task_stats:\n                task_stats[task_type] = {'success': 0, 'failed': 0, 'total': 0}\n            \n            task_stats[task_type]['total'] += 1\n            task_stats[task_type][task.status] += 1\n        \n        return {\n            'running': self._running,\n            'config': {\n                'auto_collection': self.config.enable_auto_collection,\n                'auto_analysis': self.config.enable_auto_analysis,\n                'auto_regression_detection': self.config.enable_auto_regression_detection,\n                'auto_profiling': self.config.enable_auto_profiling,\n                'auto_reporting': self.config.enable_auto_reporting\n            },\n            'scheduled_tasks': list(self._scheduled_tasks.keys()),\n            'task_history_count': len(self._task_history),\n            'recent_task_stats': task_stats,\n            'storage_paths': {\n                'baselines': str(self.baselines_path),\n                'analysis': str(self.analysis_path),\n                'alerts': str(self.alerts_path),\n                'reports': str(self.reports_path)\n            }\n        }\n\n    def get_recent_task_results(self, hours: int = 24, task_type: Optional[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"Get recent task results.\"\"\"\n        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)\n        \n        filtered_tasks = [\n            task for task in self._task_history\n            if task.end_time > cutoff_time and (task_type is None or task.task_type == task_type)\n        ]\n        \n        return [task.to_dict() for task in filtered_tasks]\n\n# Global automation instance\n_global_automation: Optional[BaselineAutomation] = None\n\ndef get_baseline_automation() -> BaselineAutomation:\n    \"\"\"Get the global baseline automation instance.\"\"\"\n    global _global_automation\n    if _global_automation is None:\n        _global_automation = BaselineAutomation()\n    return _global_automation\n\ndef set_baseline_automation(automation: BaselineAutomation) -> None:\n    \"\"\"Set the global baseline automation instance.\"\"\"\n    global _global_automation\n    _global_automation = automation\n\n# Convenience functions\nasync def start_automated_baseline_system() -> None:\n    \"\"\"Start the complete automated baseline system.\"\"\"\n    automation = get_baseline_automation()\n    await automation.start_automation()\n    logger.info(\"Started automated baseline system\")\n\nasync def stop_automated_baseline_system() -> None:\n    \"\"\"Stop the complete automated baseline system.\"\"\"\n    automation = get_baseline_automation()\n    await automation.stop_automation()\n    logger.info(\"Stopped automated baseline system\")\n\nasync def get_automation_dashboard() -> Dict[str, Any]:\n    \"\"\"Get automation dashboard data.\"\"\"\n    automation = get_baseline_automation()\n    return {\n        'status': automation.get_automation_status(),\n        'recent_tasks': automation.get_recent_task_results(hours=24),\n        'component_status': {\n            'collector': automation.collector.get_collection_status(),\n            'detector': automation.detector.get_alert_statistics(),\n            'profiler': automation.profiler.get_profiler_status()\n        }\n    }