"""Performance regression detection and alerting system."""

import asyncio
import json
import logging
import statistics
import time
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, List, Optional, Callable, Set, Tuple
import uuid

# Alerting and notifications
try:
    import smtplib
    from email.mime.text import MimeText
    from email.mime.multipart import MimeMultipart
    EMAIL_AVAILABLE = True
except ImportError:
    EMAIL_AVAILABLE = False

# Webhook notifications
try:
    import aiohttp
    WEBHOOK_AVAILABLE = True
except ImportError:
    WEBHOOK_AVAILABLE = False

from .models import (
    BaselineMetrics, RegressionAlert, AlertSeverity, MetricDefinition,
    get_metric_definition, PerformanceTrend, TrendDirection
)
from .statistical_analyzer import StatisticalAnalyzer

logger = logging.getLogger(__name__)

class RegressionThresholds:
    """Configuration for regression detection thresholds."""
    
    def __init__(
        self,
        warning_degradation_percent: float = 15.0,
        critical_degradation_percent: float = 25.0,
        emergency_degradation_percent: float = 50.0,
        minimum_samples: int = 5,
        significance_threshold: float = 0.05,
        consecutive_violations: int = 3
    ):
        """Initialize regression thresholds.
        
        Args:
            warning_degradation_percent: % degradation to trigger warning
            critical_degradation_percent: % degradation to trigger critical alert
            emergency_degradation_percent: % degradation to trigger emergency alert
            minimum_samples: Minimum samples required for detection
            significance_threshold: Statistical significance threshold
            consecutive_violations: Required consecutive violations
        """
        self.warning_degradation_percent = warning_degradation_percent
        self.critical_degradation_percent = critical_degradation_percent
        self.emergency_degradation_percent = emergency_degradation_percent
        self.minimum_samples = minimum_samples
        self.significance_threshold = significance_threshold
        self.consecutive_violations = consecutive_violations

class AlertChannel:
    """Base class for alert channels."""
    
    async def send_alert(self, alert: RegressionAlert) -> bool:
        """Send alert through this channel."""
        raise NotImplementedError

class LogAlertChannel(AlertChannel):
    """Log-based alert channel."""
    
    def __init__(self, logger_name: str = __name__):
        self.logger = logging.getLogger(logger_name)
    
    async def send_alert(self, alert: RegressionAlert) -> bool:
        """Send alert to logs."""
        try:
            log_level = {
                AlertSeverity.INFO: logging.INFO,
                AlertSeverity.WARNING: logging.WARNING,
                AlertSeverity.CRITICAL: logging.CRITICAL,
                AlertSeverity.EMERGENCY: logging.CRITICAL
            }.get(alert.severity, logging.WARNING)
            
            self.logger.log(\n                log_level,\n                f\"Performance Regression Detected: {alert.message} \"\n                f\"(Metric: {alert.metric_name}, Degradation: {alert.degradation_percentage:.1f}%)\"\n            )\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to send log alert: {e}\")\n            return False\n\nclass WebhookAlertChannel(AlertChannel):\n    \"\"\"Webhook-based alert channel.\"\"\"\n    \n    def __init__(self, webhook_url: str, timeout: int = 30):\n        self.webhook_url = webhook_url\n        self.timeout = timeout\n    \n    async def send_alert(self, alert: RegressionAlert) -> bool:\n        \"\"\"Send alert via webhook.\"\"\"\n        if not WEBHOOK_AVAILABLE:\n            logger.warning(\"Webhook alerts not available (aiohttp not installed)\")\n            return False\n        \n        try:\n            payload = {\n                'alert_type': 'performance_regression',\n                'severity': alert.severity.value,\n                'metric_name': alert.metric_name,\n                'message': alert.message,\n                'current_value': alert.current_value,\n                'baseline_value': alert.baseline_value,\n                'degradation_percentage': alert.degradation_percentage,\n                'timestamp': alert.alert_timestamp.isoformat(),\n                'alert_id': alert.alert_id,\n                'affected_operations': alert.affected_operations,\n                'probable_causes': alert.probable_causes,\n                'remediation_suggestions': alert.remediation_suggestions\n            }\n            \n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    self.webhook_url,\n                    json=payload,\n                    timeout=aiohttp.ClientTimeout(total=self.timeout)\n                ) as response:\n                    if response.status == 200:\n                        logger.info(f\"Webhook alert sent successfully for {alert.metric_name}\")\n                        return True\n                    else:\n                        logger.error(f\"Webhook alert failed with status {response.status}\")\n                        return False\n        \n        except Exception as e:\n            logger.error(f\"Failed to send webhook alert: {e}\")\n            return False\n\nclass EmailAlertChannel(AlertChannel):\n    \"\"\"Email-based alert channel.\"\"\"\n    \n    def __init__(\n        self, \n        smtp_server: str,\n        smtp_port: int,\n        username: str,\n        password: str,\n        from_email: str,\n        to_emails: List[str],\n        use_tls: bool = True\n    ):\n        self.smtp_server = smtp_server\n        self.smtp_port = smtp_port\n        self.username = username\n        self.password = password\n        self.from_email = from_email\n        self.to_emails = to_emails\n        self.use_tls = use_tls\n    \n    async def send_alert(self, alert: RegressionAlert) -> bool:\n        \"\"\"Send alert via email.\"\"\"\n        if not EMAIL_AVAILABLE:\n            logger.warning(\"Email alerts not available (smtplib not available)\")\n            return False\n        \n        try:\n            # Create email content\n            subject = f\"[{alert.severity.value.upper()}] Performance Regression: {alert.metric_name}\"\n            \n            body = f\"\"\"\nPerformance Regression Alert\n\nMetric: {alert.metric_name}\nSeverity: {alert.severity.value.upper()}\nCurrent Value: {alert.current_value:.2f}\nBaseline Value: {alert.baseline_value:.2f}\nDegradation: {alert.degradation_percentage:.1f}%\n\nMessage: {alert.message}\n\nDetection Time: {alert.detection_timestamp}\nAlert Time: {alert.alert_timestamp}\n\nAffected Operations:\n{chr(10).join('- ' + op for op in alert.affected_operations) if alert.affected_operations else '- None specified'}\n\nProbable Causes:\n{chr(10).join('- ' + cause for cause in alert.probable_causes) if alert.probable_causes else '- Analysis pending'}\n\nRemediation Suggestions:\n{chr(10).join('- ' + suggestion for suggestion in alert.remediation_suggestions) if alert.remediation_suggestions else '- Contact system administrator'}\n\nAlert ID: {alert.alert_id}\n\"\"\"\n            \n            # Create email message\n            msg = MimeMultipart()\n            msg['From'] = self.from_email\n            msg['To'] = ', '.join(self.to_emails)\n            msg['Subject'] = subject\n            msg.attach(MimeText(body, 'plain'))\n            \n            # Send email\n            with smtplib.SMTP(self.smtp_server, self.smtp_port) as server:\n                if self.use_tls:\n                    server.starttls()\n                server.login(self.username, self.password)\n                server.send_message(msg)\n            \n            logger.info(f\"Email alert sent successfully for {alert.metric_name}\")\n            return True\n        \n        except Exception as e:\n            logger.error(f\"Failed to send email alert: {e}\")\n            return False\n\nclass RegressionDetector:\n    \"\"\"Advanced performance regression detection with alerting.\n    \n    Monitors performance baselines and detects significant degradations,\n    sending alerts through configured channels.\n    \"\"\"\n    \n    def __init__(\n        self,\n        thresholds: Optional[RegressionThresholds] = None,\n        alert_channels: Optional[List[AlertChannel]] = None,\n        enable_statistical_analysis: bool = True,\n        cooldown_period: int = 300,  # 5 minutes\n        max_alerts_per_hour: int = 10\n    ):\n        \"\"\"Initialize regression detector.\n        \n        Args:\n            thresholds: Detection threshold configuration\n            alert_channels: List of alert channels to use\n            enable_statistical_analysis: Use statistical significance testing\n            cooldown_period: Minimum time between alerts for same metric (seconds)\n            max_alerts_per_hour: Maximum alerts per hour per metric\n        \"\"\"\n        self.thresholds = thresholds or RegressionThresholds()\n        self.alert_channels = alert_channels or [LogAlertChannel()]\n        self.enable_statistical_analysis = enable_statistical_analysis\n        self.cooldown_period = cooldown_period\n        self.max_alerts_per_hour = max_alerts_per_hour\n        \n        # State tracking\n        self._violation_counts: Dict[str, int] = {}  # Consecutive violations per metric\n        self._last_alert_times: Dict[str, datetime] = {}  # Last alert time per metric\n        self._alert_counts: Dict[str, List[datetime]] = {}  # Alert history per metric\n        self._active_alerts: Dict[str, RegressionAlert] = {}  # Active alerts\n        self._baseline_history: List[BaselineMetrics] = []  # Historical baselines\n        \n        # Statistical analyzer\n        if self.enable_statistical_analysis:\n            self.analyzer = StatisticalAnalyzer(\n                significance_threshold=self.thresholds.significance_threshold\n            )\n        \n        logger.info(f\"RegressionDetector initialized with {len(self.alert_channels)} channels\")\n    \n    async def check_for_regressions(\n        self,\n        current_baseline: BaselineMetrics,\n        reference_baselines: Optional[List[BaselineMetrics]] = None\n    ) -> List[RegressionAlert]:\n        \"\"\"Check current baseline for performance regressions.\n        \n        Args:\n            current_baseline: Current baseline to analyze\n            reference_baselines: Historical baselines for comparison\n            \n        Returns:\n            List of detected regression alerts\n        \"\"\"\n        alerts = []\n        \n        # Use stored history if no reference baselines provided\n        if reference_baselines is None:\n            reference_baselines = self._baseline_history[-20:]  # Last 20 baselines\n        \n        if len(reference_baselines) < self.thresholds.minimum_samples:\n            logger.debug(f\"Insufficient reference baselines ({len(reference_baselines)} < {self.thresholds.minimum_samples})\")\n            return alerts\n        \n        # Get metrics to check\n        metrics_to_check = self._get_metrics_to_check(current_baseline, reference_baselines)\n        \n        for metric_name in metrics_to_check:\n            try:\n                alert = await self._check_metric_regression(\n                    metric_name, current_baseline, reference_baselines\n                )\n                if alert:\n                    alerts.append(alert)\n            except Exception as e:\n                logger.error(f\"Error checking regression for {metric_name}: {e}\")\n        \n        # Store current baseline in history\n        self._baseline_history.append(current_baseline)\n        if len(self._baseline_history) > 100:  # Keep last 100 baselines\n            self._baseline_history = self._baseline_history[-100:]\n        \n        # Send alerts\n        for alert in alerts:\n            await self._process_alert(alert)\n        \n        return alerts\n    \n    async def _check_metric_regression(\n        self,\n        metric_name: str,\n        current_baseline: BaselineMetrics,\n        reference_baselines: List[BaselineMetrics]\n    ) -> Optional[RegressionAlert]:\n        \"\"\"Check a specific metric for regression.\"\"\"\n        \n        # Extract metric values\n        current_values = self._extract_metric_values(current_baseline, metric_name)\n        if not current_values:\n            return None\n        \n        # Get reference values\n        reference_values = []\n        for baseline in reference_baselines:\n            values = self._extract_metric_values(baseline, metric_name)\n            reference_values.extend(values)\n        \n        if len(reference_values) < self.thresholds.minimum_samples:\n            return None\n        \n        # Calculate means\n        current_mean = statistics.mean(current_values)\n        reference_mean = statistics.mean(reference_values)\n        \n        # Determine if metric should be lower or higher\n        metric_def = get_metric_definition(metric_name)\n        lower_is_better = metric_def.lower_is_better if metric_def else True\n        \n        # Calculate degradation percentage\n        if reference_mean == 0:\n            return None\n        \n        if lower_is_better:\n            # For metrics where lower is better, degradation is an increase\n            degradation_pct = ((current_mean - reference_mean) / reference_mean) * 100\n        else:\n            # For metrics where higher is better, degradation is a decrease\n            degradation_pct = ((reference_mean - current_mean) / reference_mean) * 100\n        \n        # Check if this is actually a degradation\n        if degradation_pct <= 0:\n            # Performance improved or stayed same\n            self._violation_counts[metric_name] = 0\n            return None\n        \n        # Determine severity\n        severity = self._determine_severity(degradation_pct)\n        if severity is None:\n            # Below warning threshold\n            self._violation_counts[metric_name] = 0\n            return None\n        \n        # Check for statistical significance if enabled\n        is_significant = True\n        if self.enable_statistical_analysis and len(current_values) > 1 and len(reference_values) > 1:\n            try:\n                trend = await self.analyzer.analyze_trend(\n                    metric_name, \n                    reference_baselines + [current_baseline]\n                )\n                is_significant = trend.is_significant()\n            except Exception as e:\n                logger.debug(f\"Statistical analysis failed for {metric_name}: {e}\")\n        \n        if not is_significant:\n            logger.debug(f\"Regression in {metric_name} not statistically significant\")\n            return None\n        \n        # Track consecutive violations\n        self._violation_counts[metric_name] = self._violation_counts.get(metric_name, 0) + 1\n        \n        # Check if we need consecutive violations\n        if self._violation_counts[metric_name] < self.thresholds.consecutive_violations:\n            logger.debug(f\"Regression in {metric_name}: {self._violation_counts[metric_name]}/{self.thresholds.consecutive_violations} violations\")\n            return None\n        \n        # Check cooldown period\n        if not self._is_alert_allowed(metric_name):\n            return None\n        \n        # Create regression alert\n        alert = RegressionAlert(\n            alert_id=str(uuid.uuid4()),\n            metric_name=metric_name,\n            severity=severity,\n            message=self._generate_alert_message(metric_name, degradation_pct, severity),\n            current_value=current_mean,\n            baseline_value=reference_mean,\n            threshold_value=self._get_threshold_value(degradation_pct),\n            degradation_percentage=degradation_pct,\n            detection_timestamp=current_baseline.collection_timestamp,\n            alert_timestamp=datetime.now(timezone.utc),\n            affected_operations=self._identify_affected_operations(metric_name, current_baseline),\n            probable_causes=self._identify_probable_causes(metric_name, degradation_pct),\n            remediation_suggestions=self._generate_remediation_suggestions(metric_name, severity)\n        )\n        \n        return alert\n    \n    def _extract_metric_values(self, baseline: BaselineMetrics, metric_name: str) -> List[float]:\n        \"\"\"Extract values for a specific metric from baseline.\"\"\"\n        # Use same logic as StatisticalAnalyzer\n        if metric_name == 'response_time' and baseline.response_times:\n            return baseline.response_times\n        elif metric_name == 'error_rate' and baseline.error_rates:\n            return baseline.error_rates\n        elif metric_name == 'throughput' and baseline.throughput_values:\n            return baseline.throughput_values\n        elif metric_name == 'cpu_utilization' and baseline.cpu_utilization:\n            return baseline.cpu_utilization\n        elif metric_name == 'memory_utilization' and baseline.memory_utilization:\n            return baseline.memory_utilization\n        elif metric_name == 'database_connection_time' and baseline.database_connection_time:\n            return baseline.database_connection_time\n        elif metric_name == 'cache_hit_rate' and baseline.cache_hit_rate:\n            return baseline.cache_hit_rate\n        elif metric_name in baseline.custom_metrics:\n            return baseline.custom_metrics[metric_name]\n        \n        return []\n    \n    def _get_metrics_to_check(self, current: BaselineMetrics, reference: List[BaselineMetrics]) -> List[str]:\n        \"\"\"Get list of metrics to check for regressions.\"\"\"\n        metrics = set()\n        \n        # Standard metrics\n        if current.response_times and any(b.response_times for b in reference):\n            metrics.add('response_time')\n        if current.error_rates and any(b.error_rates for b in reference):\n            metrics.add('error_rate')\n        if current.throughput_values and any(b.throughput_values for b in reference):\n            metrics.add('throughput')\n        if current.cpu_utilization and any(b.cpu_utilization for b in reference):\n            metrics.add('cpu_utilization')\n        if current.memory_utilization and any(b.memory_utilization for b in reference):\n            metrics.add('memory_utilization')\n        if current.database_connection_time and any(b.database_connection_time for b in reference):\n            metrics.add('database_connection_time')\n        if current.cache_hit_rate and any(b.cache_hit_rate for b in reference):\n            metrics.add('cache_hit_rate')\n        \n        # Custom metrics\n        for metric_name in current.custom_metrics:\n            if any(metric_name in b.custom_metrics for b in reference):\n                metrics.add(metric_name)\n        \n        return list(metrics)\n    \n    def _determine_severity(self, degradation_pct: float) -> Optional[AlertSeverity]:\n        \"\"\"Determine alert severity based on degradation percentage.\"\"\"\n        if degradation_pct >= self.thresholds.emergency_degradation_percent:\n            return AlertSeverity.EMERGENCY\n        elif degradation_pct >= self.thresholds.critical_degradation_percent:\n            return AlertSeverity.CRITICAL\n        elif degradation_pct >= self.thresholds.warning_degradation_percent:\n            return AlertSeverity.WARNING\n        else:\n            return None\n    \n    def _get_threshold_value(self, degradation_pct: float) -> float:\n        \"\"\"Get the threshold value that was exceeded.\"\"\"\n        if degradation_pct >= self.thresholds.emergency_degradation_percent:\n            return self.thresholds.emergency_degradation_percent\n        elif degradation_pct >= self.thresholds.critical_degradation_percent:\n            return self.thresholds.critical_degradation_percent\n        else:\n            return self.thresholds.warning_degradation_percent\n    \n    def _is_alert_allowed(self, metric_name: str) -> bool:\n        \"\"\"Check if alert is allowed based on cooldown and rate limits.\"\"\"\n        current_time = datetime.now(timezone.utc)\n        \n        # Check cooldown period\n        if metric_name in self._last_alert_times:\n            time_since_last = (current_time - self._last_alert_times[metric_name]).total_seconds()\n            if time_since_last < self.cooldown_period:\n                return False\n        \n        # Check rate limit (alerts per hour)\n        if metric_name not in self._alert_counts:\n            self._alert_counts[metric_name] = []\n        \n        # Remove alerts older than 1 hour\n        one_hour_ago = current_time - timedelta(hours=1)\n        self._alert_counts[metric_name] = [\n            alert_time for alert_time in self._alert_counts[metric_name]\n            if alert_time > one_hour_ago\n        ]\n        \n        # Check if under rate limit\n        if len(self._alert_counts[metric_name]) >= self.max_alerts_per_hour:\n            return False\n        \n        return True\n    \n    def _generate_alert_message(self, metric_name: str, degradation_pct: float, severity: AlertSeverity) -> str:\n        \"\"\"Generate human-readable alert message.\"\"\"\n        metric_display = metric_name.replace('_', ' ').title()\n        \n        if severity == AlertSeverity.EMERGENCY:\n            return f\"EMERGENCY: {metric_display} has severely degraded by {degradation_pct:.1f}%\"\n        elif severity == AlertSeverity.CRITICAL:\n            return f\"CRITICAL: {metric_display} has critically degraded by {degradation_pct:.1f}%\"\n        else:\n            return f\"WARNING: {metric_display} has degraded by {degradation_pct:.1f}%\"\n    \n    def _identify_affected_operations(self, metric_name: str, baseline: BaselineMetrics) -> List[str]:\n        \"\"\"Identify operations affected by the regression.\"\"\"\n        affected = []\n        \n        # Look for operation-specific metrics in custom metrics\n        for custom_metric in baseline.custom_metrics:\n            if metric_name in custom_metric and '_' in custom_metric:\n                operation = custom_metric.split('_')[0]\n                if operation not in affected:\n                    affected.append(operation)\n        \n        # Add general categories based on metric type\n        if metric_name in ['response_time', 'database_connection_time']:\n            affected.extend(['API endpoints', 'Database operations'])\n        elif metric_name == 'error_rate':\n            affected.extend(['All operations', 'User requests'])\n        elif metric_name in ['cpu_utilization', 'memory_utilization']:\n            affected.extend(['System performance', 'All operations'])\n        elif metric_name == 'cache_hit_rate':\n            affected.extend(['Cache operations', 'Data retrieval'])\n        \n        return list(set(affected))  # Remove duplicates\n    \n    def _identify_probable_causes(self, metric_name: str, degradation_pct: float) -> List[str]:\n        \"\"\"Identify probable causes of the regression.\"\"\"\n        causes = []\n        \n        # General causes based on metric type\n        if metric_name == 'response_time':\n            causes.extend([\n                'Increased load or traffic',\n                'Database performance issues',\n                'Network latency',\n                'Resource contention',\n                'Inefficient code changes'\n            ])\n        elif metric_name == 'error_rate':\n            causes.extend([\n                'Recent code deployment',\n                'External service failures',\n                'Database connectivity issues',\n                'Configuration changes',\n                'Resource exhaustion'\n            ])\n        elif metric_name in ['cpu_utilization', 'memory_utilization']:\n            causes.extend([\n                'Increased workload',\n                'Memory leaks',\n                'Inefficient algorithms',\n                'Resource-intensive operations',\n                'Background processes'\n            ])\n        elif metric_name == 'database_connection_time':\n            causes.extend([\n                'Database server overload',\n                'Network connectivity issues',\n                'Database configuration changes',\n                'Connection pool exhaustion'\n            ])\n        elif metric_name == 'cache_hit_rate':\n            causes.extend([\n                'Cache invalidation issues',\n                'Memory pressure',\n                'Cache configuration changes',\n                'Data access pattern changes'\n            ])\n        \n        # Severity-based causes\n        if degradation_pct > 50:\n            causes.extend([\n                'System outage or partial failure',\n                'Critical resource exhaustion',\n                'Major configuration error'\n            ])\n        \n        return causes\n    \n    def _generate_remediation_suggestions(self, metric_name: str, severity: AlertSeverity) -> List[str]:\n        \"\"\"Generate remediation suggestions.\"\"\"\n        suggestions = []\n        \n        # Immediate actions based on severity\n        if severity == AlertSeverity.EMERGENCY:\n            suggestions.extend([\n                'Investigate immediately - system may be in critical state',\n                'Consider rolling back recent changes',\n                'Check system resource availability',\n                'Escalate to on-call engineer'\n            ])\n        elif severity == AlertSeverity.CRITICAL:\n            suggestions.extend([\n                'Investigate within 30 minutes',\n                'Review recent deployments',\n                'Check system metrics and logs'\n            ])\n        \n        # Metric-specific suggestions\n        if metric_name == 'response_time':\n            suggestions.extend([\n                'Review database query performance',\n                'Check for resource bottlenecks',\n                'Analyze request patterns',\n                'Consider scaling resources'\n            ])\n        elif metric_name == 'error_rate':\n            suggestions.extend([\n                'Check application logs for error patterns',\n                'Verify external service status',\n                'Review recent code changes',\n                'Test critical user flows'\n            ])\n        elif metric_name in ['cpu_utilization', 'memory_utilization']:\n            suggestions.extend([\n                'Monitor resource usage trends',\n                'Identify resource-intensive processes',\n                'Consider horizontal scaling',\n                'Review application efficiency'\n            ])\n        \n        # General suggestions\n        suggestions.extend([\n            'Monitor metric trends for recovery',\n            'Document findings for future reference',\n            'Update monitoring thresholds if needed'\n        ])\n        \n        return suggestions\n    \n    async def _process_alert(self, alert: RegressionAlert) -> None:\n        \"\"\"Process and send an alert through all channels.\"\"\"\n        # Update tracking\n        self._last_alert_times[alert.metric_name] = alert.alert_timestamp\n        if alert.metric_name not in self._alert_counts:\n            self._alert_counts[alert.metric_name] = []\n        self._alert_counts[alert.metric_name].append(alert.alert_timestamp)\n        \n        # Store active alert\n        self._active_alerts[alert.alert_id] = alert\n        \n        # Send through all channels\n        for channel in self.alert_channels:\n            try:\n                success = await channel.send_alert(alert)\n                if success:\n                    logger.info(f\"Alert sent via {type(channel).__name__} for {alert.metric_name}\")\n                else:\n                    logger.error(f\"Failed to send alert via {type(channel).__name__}\")\n            except Exception as e:\n                logger.error(f\"Error sending alert via {type(channel).__name__}: {e}\")\n    \n    async def resolve_alert(self, alert_id: str, resolution_note: str = \"\") -> bool:\n        \"\"\"Mark an alert as resolved.\"\"\"\n        if alert_id in self._active_alerts:\n            alert = self._active_alerts[alert_id]\n            alert.metadata['resolved'] = True\n            alert.metadata['resolution_time'] = datetime.now(timezone.utc).isoformat()\n            alert.metadata['resolution_note'] = resolution_note\n            \n            # Reset violation count for this metric\n            self._violation_counts[alert.metric_name] = 0\n            \n            logger.info(f\"Alert {alert_id} resolved for metric {alert.metric_name}\")\n            return True\n        \n        return False\n    \n    def get_active_alerts(self) -> List[RegressionAlert]:\n        \"\"\"Get all active alerts.\"\"\"\n        return [\n            alert for alert in self._active_alerts.values()\n            if not alert.metadata.get('resolved', False)\n        ]\n    \n    def get_alert_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about alert activity.\"\"\"\n        current_time = datetime.now(timezone.utc)\n        \n        # Count alerts in last 24 hours\n        alerts_24h = 0\n        for alert_times in self._alert_counts.values():\n            alerts_24h += len([\n                t for t in alert_times\n                if (current_time - t).total_seconds() < 24 * 3600\n            ])\n        \n        return {\n            'active_alerts': len(self.get_active_alerts()),\n            'total_alerts_24h': alerts_24h,\n            'metrics_with_violations': len([\n                metric for metric, count in self._violation_counts.items()\n                if count > 0\n            ]),\n            'alert_channels': len(self.alert_channels),\n            'violation_counts': self._violation_counts.copy(),\n            'thresholds': {\n                'warning': self.thresholds.warning_degradation_percent,\n                'critical': self.thresholds.critical_degradation_percent,\n                'emergency': self.thresholds.emergency_degradation_percent\n            }\n        }\n\n# Global detector instance\n_global_detector: Optional[RegressionDetector] = None\n\ndef get_regression_detector() -> RegressionDetector:\n    \"\"\"Get the global regression detector instance.\"\"\"\n    global _global_detector\n    if _global_detector is None:\n        _global_detector = RegressionDetector()\n    return _global_detector\n\ndef set_regression_detector(detector: RegressionDetector) -> None:\n    \"\"\"Set the global regression detector instance.\"\"\"\n    global _global_detector\n    _global_detector = detector\n\n# Convenience functions\nasync def check_baseline_for_regressions(\n    baseline: BaselineMetrics,\n    reference_baselines: Optional[List[BaselineMetrics]] = None\n) -> List[RegressionAlert]:\n    \"\"\"Check a baseline for regressions using the global detector.\"\"\"\n    detector = get_regression_detector()\n    return await detector.check_for_regressions(baseline, reference_baselines)\n\nasync def setup_webhook_alerts(webhook_url: str) -> None:\n    \"\"\"Setup webhook alerts for the global detector.\"\"\"\n    detector = get_regression_detector()\n    webhook_channel = WebhookAlertChannel(webhook_url)\n    detector.alert_channels.append(webhook_channel)\n    logger.info(f\"Added webhook alert channel: {webhook_url}\")